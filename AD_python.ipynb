{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/spotify_songs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)\n",
    "display(data.describe().T.style.background_gradient(cmap='YlGnBu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "display(missing_values)\n",
    "\n",
    "# display the lines with missing values\n",
    "missing_data = data[data.isnull().any(axis=1)]\n",
    "display(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove id columns\n",
    "data = data.drop(columns=['track_id', 'track_album_id', \"playlist_id\"], axis=1)\n",
    "# drop the missing values\n",
    "data = data.dropna()\n",
    "display(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform track_album_release_date into datetime\n",
    "data['track_album_release_date'] = pd.to_datetime(data['track_album_release_date'], format='mixed')\n",
    "\n",
    "# transform categorical columns into categorical data type\n",
    "categorical_cols = ['playlist_genre', 'playlist_subgenre', 'track_artist', 'playlist_name', 'track_album_name']\n",
    "for col in categorical_cols:\n",
    "    data[col] = data[col].astype('category')\n",
    "\n",
    "# transform the duration_ms into minutes\n",
    "data['duration_s'] = data['duration_ms'] / 1000\n",
    "data.drop(columns=['duration_ms'], inplace=True)\n",
    "\n",
    "# For numeric columns that represent discrete values (like key and mode), convert to categorical\n",
    "key_mapping = {\n",
    "    0: 'C', 1: 'C♯/D♭', 2: 'D', 3: 'D♯/E♭', 4: 'E', 5: 'F',\n",
    "    6: 'F♯/G♭', 7: 'G', 8: 'G♯/A♭', 9: 'A', 10: 'A♯/B♭', 11: 'B'\n",
    "}\n",
    "data['key'] = data['key'].map(key_mapping).astype('category')\n",
    "\n",
    "data['mode'] = data['mode'].map({0: 'Minor', 1: 'Major'}).astype('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "duplicates = data.duplicated().sum()\n",
    "display(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Réduction de dimension par ACP (Analyse en Composantes Principales)\n",
    "Dans cette partie, nous allons effectuer une analyse en composantes principales (ACP) sur les données prétraitées. L'ACP est une technique de réduction de dimension qui permet de projeter les données d'origine dans un espace de dimension inférieure.\n",
    "Nous avons gardé 20 variables et nous allons étudier s'il est possible de réduire la dimensionnalité de ces données tout en préservant un maximum d'information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Format des données\n",
    "Ici on ne sélectionne que les variables quantitatives, en y ajoutant une variable qualitative (`playlist_genre`) pour voir si elle a un impact sur la projection des données. On garde au final 11 variables quantitatives et 1 variable qualitative.\n",
    "\n",
    "On décide de normaliser les données pour que chaque variable ait une moyenne de 0 et un écart-type de 1. Cela est important car l'ACP est sensible à l'échelle des variables. On utilise la méthode `StandardScaler` de `sklearn` pour normaliser les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the quantitative columns\n",
    "\n",
    "qualisup = 'playlist_genre'\n",
    "\n",
    "data_quanti = data.select_dtypes(include=['int64', 'float64'])\n",
    "data_quanti[qualisup] = data[qualisup]\n",
    "data_quanti = data_quanti.set_index(qualisup)\n",
    "\n",
    "data_quanti.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quanti[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import prince \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "normalize_bool = True\n",
    "\n",
    "if normalize_bool:\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data_quanti)\n",
    "else:\n",
    "    data_scaled = data_quanti.values\n",
    "\n",
    "pca = PCA(\n",
    "    n_components=10,  # Number of components to keep\n",
    "    random_state=1  # For reproducibility\n",
    ")\n",
    "\n",
    "projected = pca.fit_transform(data_scaled)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "eig = pd.DataFrame(\n",
    "    {\n",
    "        \"Dimension\" : [f\"PC{i+1}\" for i in range(len(explained_variance))],\n",
    "        \"Variance\" : np.round(pca.explained_variance_, 2),\n",
    "        \"% explained variance\" : np.round(explained_variance*100, 1),\n",
    "        \"% cumulative variance\" : np.round(np.cumsum(explained_variance)*100, 1)\n",
    "    }\n",
    ")\n",
    "eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.bar(range(10), pca.explained_variance_ratio_[:10]*100, align='center',\n",
    "        color='coral', ecolor='black')\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_ylabel(\"Variance\")\n",
    "ax.set_title(\"\", fontsize=35)\n",
    "ax.set_title(u\"Pourcentage de variance expliqué\", fontsize=20)\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.plot(np.cumsum(pca.explained_variance_ratio_), color='coral', marker='o')\n",
    "ax.hlines(0.80, 0, 10, colors='grey', linestyles='dashed', alpha=0.5)\n",
    "ax.set_title(u'Pourcentage de variance expliqué cumulé', fontsize=20)\n",
    "\n",
    "fig.suptitle(u\"Résultat ACP\", fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation :** \n",
    "\n",
    "L’analyse de la variance expliquée montre que les **7 premières composantes principales** permettent de **représenter 80,1 % de la variance totale** du jeu de données. Cela signifie que l’essentiel de l’information contenue dans les **11 variables numériques initiales** (comme *danceability*, *energy*, *speechiness*, *tempo*, etc.) peut être résumé avec seulement 7 dimensions, ce qui représente une **réduction significative de la complexité** du dataset tout en conservant une bonne qualité descriptive.\n",
    "\n",
    "Dans cette optique de réduction de dimension, il serait **pertinent de conserver ces 7 composantes principales** pour la suite des analyses (clustering, visualisation, classification), car elles capturent la structure principale des données tout en éliminant le \"bruit\".\n",
    "\n",
    "Dans l’analyse factorielle, nous avons choisi d’**interpréter les trois premières composantes principales**, qui à elles seules expliquent 45 % de la variance totale. Elles offrent un bon compromis entre lisibilité et pertinence pour une visualisation ou une première analyse des relations entre les variables et les genres musicaux (*playlist_genre*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "box=plt.boxplot(projected[:,0:3],whis=100)\n",
    "plt.title(u\"Distribution des premières composantes\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation :**\n",
    "- Le graphique ci-dessus montre la distribution des premières composantes principales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_prince = prince.PCA(\n",
    "    n_components=3,\n",
    "    n_iter=3,\n",
    "    rescale_with_mean=normalize_bool,\n",
    "    rescale_with_std=normalize_bool,\n",
    "    copy=True,\n",
    "    check_input=True,\n",
    "    engine='sklearn',\n",
    "    random_state=1\n",
    ")\n",
    "pca_prince = pca_prince.fit(\n",
    "    data_quanti,\n",
    "    sample_weight=None,\n",
    "    column_weight=None,\n",
    "    supplementary_columns=None\n",
    ")\n",
    "\n",
    "# Create a correlation matrix of the PCA compnenets\n",
    "correlations = pca_prince.column_correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlations, annot=True, cmap='RdBu', center=0)\n",
    "plt.title('Corrélations entre les variables et les composantes principales')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des coordonnées des variables (correlations avec les composantes principales)\n",
    "coords = pca_prince.column_correlations\n",
    "\n",
    "# Création de la figure et des sous-graphiques\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "# Boucle pour créer les trois graphiques\n",
    "for idx, (ax, (x_comp, y_comp)) in enumerate(zip(axes, [(0, 1), (1, 2), (0, 2)])):\n",
    "    ax.grid(False)\n",
    "    for i in range(coords.shape[0]):\n",
    "        ax.arrow(0, 0, coords.iloc[i, x_comp], coords.iloc[i, y_comp], \n",
    "                 color='black', alpha=0.7, head_width=0.02, head_length=0.03)\n",
    "        ax.text(coords.iloc[i, x_comp] * 1.1, coords.iloc[i, y_comp] * 1.1, \n",
    "                coords.index[i], color='black', ha='center', va='center')\n",
    "\n",
    "    # Ajout du cercle de rayon 1\n",
    "    ax.add_artist(plt.Circle((0, 0), radius=1, color='cornflowerblue', fill=False))\n",
    "\n",
    "    # Ajout des axes\n",
    "    ax.axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "    ax.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # Paramètres du graphique\n",
    "    ax.set_xlim(-1.1, 1.1)\n",
    "    ax.set_ylim(-1.1, 1.1)\n",
    "    ax.set_xlabel(f'Composante principale {x_comp + 1}')\n",
    "    ax.set_ylabel(f'Composante principale {y_comp + 1}')\n",
    "    ax.set_title(f'Projection des variables: CP{x_comp + 1} vs CP{y_comp + 1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La table des correlations et les trois graphiques ci-dessus représentent les projections des features sur les trois premières composantes principales nous donne des informations sur la structure des données réduites.\n",
    "\n",
    "- **Composante principale 1** : \n",
    "    - Les variables `energy (-0.91)`, `loudness (-0.80)` et `acousticness (+0.72)` sont linéairement corrélées avec la première composante principale, en soulignant que `energy` et `loudness` sont inversément corrélées avec `acousticness`. Cela indique que la CP1 oppose les morceaux **énergiques, forts en volume et peu acoustiques** (ex : rock, électro) aux morceaux **calmes, acoustiques et peu énergétiques** (ex : folk, classique).\n",
    "- **Composante principale 2** : Sur le graphique de gauche, on remarque une opposition des variables `instrumentalness (+0.45)`, `duration_s (+0.38)` contre `danceability (-0.68)`, `valence (-0.62)`, `track_popularity (-0.37)`, `speechiness (-0.39)`. Cette composante principale oppose deux profils de morceaux :\n",
    "    - D’un côté, les morceaux **instrumentaux, longs et peu populaires** (forte contribution de `instrumentalness` et `duration_s`), souvent associés à des genres comme le classique ou le jazz.\n",
    "    - De l’autre, les chansons **courtes, dansantes, joyeuses et populaires** (forte contribution de `danceability`, `valence` et `track_popularity`), typiques de la pop ou de la musique de club. \n",
    "    - Enfin, cette opposition suggère que les morceaux avec des paroles marquées (`speechiness`) et une structure rythmique engageante (`danceability`) sont plus susceptibles de générer de la popularité.\n",
    "- **Composantes principales 2 et 3** : Sur le graphique au centre, on peut extraire plusieurs informations :\n",
    "    - Les morceaux dansants et joyeux (haute valence) s'opposent dans une moindre mesure aux morceaux de faible tempo. De plus, ces morceaux semblent avoir peu de versions live.\n",
    "    - On observe aussi que les morceaux instrumentaux et longs ont généralement une faible popularité, tandis que les morceaux courts et peu instrumentaux sont souvent plus populaires, ce qui est typique de la musique pop, qui est souvent plus accessible et commerciale.\n",
    "- **Composante principale 3** : La troisième composante (CP3) révèle un paradoxe : elle regroupe des morceaux à fort potentiel dansant (`danceability`) et mood positif (`valence`), mais qui restent peu populaires (`track_popularity`). Ces morceaux sont souvent instrumentaux (`instrumentalness`), longs (`duration_s`) et à tempo faible (`tempo`), ce qui les éloigne des standards des charts. Cette composante pourrait représenter des créations artistiques équilibrant danse et complexité, mais peinant à atteindre un large public.\n",
    "\n",
    "Pour mieux comprendre à quoi correspond les type morceaux extraits par ces composantes principales, nous allons regarder les morceaux (individus) contribuant le plus à chacune des composantes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display la mediane de notre dataset quantitatif à fins de comparaison\n",
    "median_values = data_quanti.median()\n",
    "median_values = median_values.to_frame(name='median')\n",
    "median_values = median_values.reset_index()\n",
    "median_values.columns = ['feature', 'median']\n",
    "median_values['feature'] = median_values['feature'].astype('category')\n",
    "median_values = median_values.set_index('feature')\n",
    "median_values = median_values.sort_index()\n",
    "median_values = median_values.T\n",
    "median_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the contributions of individuals to the first principal component\n",
    "contributions = projected[:, 0]\n",
    "\n",
    "# Get the indices of the top 5 positive contributors\n",
    "top_5_positive_indices = np.argsort(contributions)[-5:]\n",
    "\n",
    "# Get the indices of the top 5 negative contributors\n",
    "top_5_negative_indices = np.argsort(contributions)[:5]\n",
    "\n",
    "# Extract the corresponding rows from the original dataset for positive contributions\n",
    "top_5_positive_tracks = data_quanti.iloc[top_5_positive_indices].copy()\n",
    "top_5_positive_tracks['track_name'] = data.iloc[top_5_positive_indices]['track_name'].values\n",
    "top_5_positive_tracks['track_artist'] = data.iloc[top_5_positive_indices]['track_artist'].values\n",
    "top_5_positive_tracks['contribution'] = contributions[top_5_positive_indices]\n",
    "\n",
    "# Extract the corresponding rows from the original dataset for negative contributions\n",
    "top_5_negative_tracks = data_quanti.iloc[top_5_negative_indices].copy()\n",
    "top_5_negative_tracks['track_name'] = data.iloc[top_5_negative_indices]['track_name'].values\n",
    "top_5_negative_tracks['track_artist'] = data.iloc[top_5_negative_indices]['track_artist'].values\n",
    "top_5_negative_tracks['contribution'] = contributions[top_5_negative_indices]\n",
    "\n",
    "# Display the top 5 positive and negative tracks with additional information\n",
    "print(\"Top 5 Positive Contributions (PC1):\")\n",
    "display(top_5_positive_tracks)\n",
    "\n",
    "print(\"Top 5 Negative Contributions (PC1) :\")\n",
    "display(top_5_negative_tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation de l'Analyse en Composantes Principales (PCA)\n",
    "\n",
    "\n",
    "#### **Analyse des principales corrélations :**\n",
    "- **PC1 (Composante Principale 1) :**  \n",
    "  - **Corrélations négatives fortes :** `energy (-0.91)`, `loudness (-0.80)`.  \n",
    "    → Cette composante sépare les morceaux énergiques et bruyants des morceaux plus doux.  \n",
    "  - **Corrélation positive forte :** `acousticness (+0.72)`.  \n",
    "    → Elle oppose également les morceaux acoustiques aux morceaux énergétiques.  \n",
    "\n",
    "- **PC2 (Composante Principale 2) :**  \n",
    "  - **Corrélations négatives :** `danceability (-0.68)`, `valence (-0.62)`, `track_popularity (-0.37)`, `speechiness (-0.39)`.  \n",
    "    → Cette composante semble regrouper les morceaux dansants\n",
    "  - **Corrélations positives :** `instrumentalness (+0.45)`, `duration_s (+0.38)`.  \n",
    "    → Elle représente les morceaux plus longs et instrumentaux.  \n",
    "  → Cette dimension est donc liée à l'humeur des morceaux, opposant les morceaux joyeux et dansants aux morceaux plus sombres et instrumentaux.\n",
    "\n",
    "- **PC3 (Composante Principale 3) :**  \n",
    "  - **Corrélations positives :** `danceability (+0.46)`, `instrumentalness (+0.39)`, `duration_s (+0.45)`.  \n",
    "    → Cette dimension semble être liée à la dansabilité et à la durée des morceaux.  \n",
    "  - **Corrélation négative :** `tempo (-0.35)`.  \n",
    "    → Elle oppose les morceaux rapides et lents.  \n",
    "\n",
    "- **PC4 (Composante Principale 4) :**  \n",
    "  - **Corrélation positive forte :** `speechiness (+0.71)`.  \n",
    "    → Cette composante est dominée par la présence de paroles (ex : rap).  \n",
    "  - **Corrélation positive :** `liveness (+0.58)`.  \n",
    "    → Elle met en avant les morceaux enregistrés en live.  \n",
    "\n",
    "### **Interprétation globale :**\n",
    "- **PC1 sépare les morceaux énergiques et bruyants des morceaux acoustiques.**  \n",
    "- **PC2 distingue les morceaux dansants et joyeux des morceaux instrumentaux et longs.**  \n",
    "- **PC3 différencie les morceaux en fonction de leur dansabilité et de leur tempo.**  \n",
    "- **PC4 met en avant les morceaux avec beaucoup de paroles et ceux enregistrés en live.**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/pmca/plot_pca.py\n",
    "def plot_pca(ax1=0, ax2=1, pca=pca, data=data_quanti):\n",
    "  dataset = pca.transform(data)\n",
    "  dataset.reset_index(inplace=True)\n",
    "  sns.scatterplot(data = dataset,\n",
    "                  x = ax1, y = ax2,\n",
    "                  hue = qualisup, alpha=.3)\n",
    "\n",
    "  plt.xlabel('Component {} — {:.2f}%'.format(ax1, pca.percentage_of_variance_[ax1]))\n",
    "  plt.ylabel('Component {} — {:.2f}%'.format(ax2, pca.percentage_of_variance_[ax2]))\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "plot_pca(0,1)\n",
    "plot_pca(1,2)\n",
    "plot_pca(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
