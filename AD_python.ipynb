{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/spotify_songs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)\n",
    "display(data.describe().T.style.background_gradient(cmap='YlGnBu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "display(missing_values)\n",
    "\n",
    "# display the lines with missing values\n",
    "missing_data = data[data.isnull().any(axis=1)]\n",
    "display(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove id columns\n",
    "data = data.drop(columns=['track_id', 'track_album_id', \"playlist_id\"], axis=1)\n",
    "# drop the missing values\n",
    "data = data.dropna()\n",
    "display(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform track_album_release_date into datetime\n",
    "data['track_album_release_date'] = pd.to_datetime(data['track_album_release_date'], format='mixed')\n",
    "\n",
    "# transform categorical columns into categorical data type\n",
    "categorical_cols = ['playlist_genre', 'playlist_subgenre', 'track_artist', 'playlist_name', 'track_album_name']\n",
    "for col in categorical_cols:\n",
    "    data[col] = data[col].astype('category')\n",
    "\n",
    "# transform the duration_ms into minutes\n",
    "data['duration_s'] = data['duration_ms'] / 1000\n",
    "data.drop(columns=['duration_ms'], inplace=True)\n",
    "\n",
    "# For numeric columns that represent discrete values (like key and mode), convert to categorical\n",
    "key_mapping = {\n",
    "    0: 'C', 1: 'C♯/D♭', 2: 'D', 3: 'D♯/E♭', 4: 'E', 5: 'F',\n",
    "    6: 'F♯/G♭', 7: 'G', 8: 'G♯/A♭', 9: 'A', 10: 'A♯/B♭', 11: 'B'\n",
    "}\n",
    "data['key'] = data['key'].map(key_mapping).astype('category')\n",
    "\n",
    "data['mode'] = data['mode'].map({0: 'Minor', 1: 'Major'}).astype('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "duplicates = data.duplicated().sum()\n",
    "display(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = data.select_dtypes(include='number')\n",
    "cat = data.select_dtypes(include='category')\n",
    "print('Data quantitative :', round(100*num.shape[1]/data.shape[1], 2), '%')\n",
    "print('Data qualitative :', round(100*cat.shape[1]/data.shape[1], 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copier data dans data_songs\n",
    "data_songs = data.copy()\n",
    "\n",
    "# observer les données dupliquées sur toutes les colonnes\n",
    "duplicates = data_songs.duplicated().sum()\n",
    "print(f'Number of duplicates: {duplicates}')\n",
    "\n",
    "# Nombre de valeurs dupliquées avant suppression\n",
    "initial_duplicates = data_songs.duplicated(subset=['track_artist', 'track_name']).sum()\n",
    "print(f'Initial number of duplicates: {initial_duplicates}')\n",
    "\n",
    "# Supprimer les colonnes 'playlist'\n",
    "data_songs = data_songs.drop(columns=['playlist_genre', 'playlist_subgenre', 'playlist_name'])\n",
    "\n",
    "# Regarder le nombre de doublons après suppression\n",
    "duplicates = data_songs.duplicated().sum()\n",
    "print(f'Number of duplicates - après suppression des playlists : {duplicates}')\n",
    "\n",
    "# Supprimer les doublons\n",
    "data_songs = data_songs.drop_duplicates()\n",
    "\n",
    "# regarder le nombre de doublons en fonction de 'track_artist' et 'track_name'\n",
    "duplicates = data_songs.duplicated(subset=['track_artist', 'track_name']).sum()\n",
    "print(f'Number of duplicates - en se basant sur le nom d artiste de la track et : {duplicates}')\n",
    "\n",
    "# garder dans data_songs les lignes dupliquées ayant la popularité la plus élevée\n",
    "data_songs = data_songs.sort_values('track_popularity', ascending=False).drop_duplicates(subset=['track_artist', 'track_name'])\n",
    "# regarder le nombre de doublons en fonction de 'track_artist' et 'track_name'\n",
    "duplicates = data_songs.duplicated().sum()\n",
    "print(f'Number of duplicates - après suppression des supposés doublons : {duplicates}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier data_songs par l'index 'idx'\n",
    "data_songs.sort_index(inplace=True)\n",
    "\n",
    "display(data_songs[-5:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Réduction de dimension par ACP (Analyse en Composantes Principales)\n",
    "Dans cette partie, nous allons effectuer une analyse en composantes principales (ACP) sur les données prétraitées. L'ACP est une technique de réduction de dimension qui permet de projeter les données d'origine dans un espace de dimension inférieure.\n",
    "Nous avons gardé 20 variables et nous allons étudier s'il est possible de réduire la dimensionnalité de ces données tout en préservant un maximum d'information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Format des données\n",
    "Ici on ne sélectionne que les variables quantitatives, en y ajoutant une variable qualitative (`playlist_genre`) pour voir si elle a un impact sur la projection des données. On garde au final 11 variables quantitatives et 1 variable qualitative.\n",
    "\n",
    "On décide de normaliser les données pour que chaque variable ait une moyenne de 0 et un écart-type de 1. Cela est important car l'ACP est sensible à l'échelle des variables. On utilise la méthode `StandardScaler` de `sklearn` pour normaliser les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the quantitative columns\n",
    "\n",
    "qualisup = 'playlist_genre'\n",
    "\n",
    "data_quanti = data.select_dtypes(include=['int64', 'float64'])\n",
    "data_quanti[qualisup] = data[qualisup]\n",
    "data_quanti = data_quanti.set_index(qualisup)\n",
    "\n",
    "data_quanti.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quanti.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Application de l'ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import prince \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "normalize_bool = True\n",
    "\n",
    "if normalize_bool:\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data_quanti)\n",
    "else:\n",
    "    data_scaled = data_quanti.values\n",
    "\n",
    "pca = PCA(\n",
    "    n_components=10,  # Number of components to keep\n",
    "    random_state=1  # For reproducibility\n",
    ")\n",
    "\n",
    "projected = pca.fit_transform(data_scaled)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "eig = pd.DataFrame(\n",
    "    {\n",
    "        \"Dimension\" : [f\"PC{i+1}\" for i in range(len(explained_variance))],\n",
    "        \"Variance\" : np.round(pca.explained_variance_, 2),\n",
    "        \"% explained variance\" : np.round(explained_variance*100, 1),\n",
    "        \"% cumulative variance\" : np.round(np.cumsum(explained_variance)*100, 1)\n",
    "    }\n",
    ")\n",
    "eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.bar(range(10), pca.explained_variance_ratio_[:10]*100, align='center',\n",
    "        color='coral', ecolor='black')\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_ylabel(\"Variance\")\n",
    "ax.set_title(\"\", fontsize=35)\n",
    "ax.set_title(u\"Pourcentage de variance expliqué\", fontsize=20)\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.plot(np.cumsum(pca.explained_variance_ratio_), color='coral', marker='o')\n",
    "ax.hlines(0.80, 0, 10, colors='grey', linestyles='dashed', alpha=0.5)\n",
    "ax.set_title(u'Pourcentage de variance expliqué cumulé', fontsize=20)\n",
    "\n",
    "fig.suptitle(u\"Résultat ACP\", fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation :** \n",
    "\n",
    "L’analyse de la variance expliquée montre que les **7 premières composantes principales** permettent de **représenter 80,1 % de la variance totale** du jeu de données. Cela signifie que l’essentiel de l’information contenue dans les **11 variables numériques initiales** (comme *danceability*, *energy*, *speechiness*, *tempo*, etc.) peut être résumé avec seulement 7 dimensions, ce qui représente une **réduction significative de la complexité** du dataset tout en conservant une bonne qualité descriptive.\n",
    "\n",
    "Dans cette optique de réduction de dimension, il serait **pertinent de conserver ces 7 composantes principales** pour la suite des analyses (clustering, visualisation, classification), car elles capturent la structure principale des données tout en éliminant le \"bruit\".\n",
    "\n",
    "Dans l’analyse factorielle, nous avons choisi d’**interpréter les trois premières composantes principales**, qui à elles seules expliquent 45 % de la variance totale. Elles offrent un bon compromis entre lisibilité et pertinence pour une visualisation ou une première analyse des relations entre les variables et les genres musicaux (*playlist_genre*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "box=plt.boxplot(projected[:,0:3],whis=100)\n",
    "plt.title(u\"Distribution des premières composantes\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation :**\n",
    "- Le graphique ci-dessus montre la distribution des premières composantes principales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_prince = prince.PCA(\n",
    "    n_components=3,\n",
    "    n_iter=3,\n",
    "    rescale_with_mean=normalize_bool,\n",
    "    rescale_with_std=normalize_bool,\n",
    "    copy=True,\n",
    "    check_input=True,\n",
    "    engine='sklearn',\n",
    "    random_state=1\n",
    ")\n",
    "pca_prince = pca_prince.fit(\n",
    "    data_quanti,\n",
    "    sample_weight=None,\n",
    "    column_weight=None,\n",
    "    supplementary_columns=None\n",
    ")\n",
    "\n",
    "# Create a correlation matrix of the PCA compnenets\n",
    "correlations = pca_prince.column_correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlations, annot=True, cmap='RdBu', center=0)\n",
    "plt.title('Corrélations entre les variables et les composantes principales')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des coordonnées des variables (correlations avec les composantes principales)\n",
    "coords = pca_prince.column_correlations\n",
    "\n",
    "# Création de la figure et des sous-graphiques\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "# Boucle pour créer les trois graphiques\n",
    "for idx, (ax, (x_comp, y_comp)) in enumerate(zip(axes, [(0, 1), (1, 2), (0, 2)])):\n",
    "    ax.grid(False)\n",
    "    for i in range(coords.shape[0]):\n",
    "        ax.arrow(0, 0, coords.iloc[i, x_comp], coords.iloc[i, y_comp], \n",
    "                 color='black', alpha=0.7, head_width=0.02, head_length=0.03)\n",
    "        ax.text(coords.iloc[i, x_comp] * 1.1, coords.iloc[i, y_comp] * 1.1, \n",
    "                coords.index[i], color='black', ha='center', va='center')\n",
    "\n",
    "    # Ajout du cercle de rayon 1\n",
    "    ax.add_artist(plt.Circle((0, 0), radius=1, color='cornflowerblue', fill=False))\n",
    "\n",
    "    # Ajout des axes\n",
    "    ax.axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "    ax.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # Paramètres du graphique\n",
    "    ax.set_xlim(-1.1, 1.1)\n",
    "    ax.set_ylim(-1.1, 1.1)\n",
    "    ax.set_xlabel(f'Composante principale {x_comp + 1}')\n",
    "    ax.set_ylabel(f'Composante principale {y_comp + 1}')\n",
    "    ax.set_title(f'Projection des variables: CP{x_comp + 1} vs CP{y_comp + 1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La table des correlations et les trois graphiques ci-dessus représentent les projections des features sur les trois premières composantes principales nous donne des informations sur la structure des données réduites.\n",
    "\n",
    "- **Composante principale 1** : \n",
    "    - Les variables `energy (-0.91)`, `loudness (-0.80)` et `acousticness (+0.72)` sont linéairement corrélées avec la première composante principale, en soulignant que `energy` et `loudness` sont inversément corrélées avec `acousticness`. Cela indique que la CP1 oppose les morceaux **énergiques, forts en volume et peu acoustiques** (ex : rock, électro) aux morceaux **calmes, acoustiques et peu énergétiques** (ex : folk, classique).\n",
    "- **Composante principale 2** : Sur le graphique de gauche, on remarque une opposition des variables `instrumentalness (+0.45)`, `duration_s (+0.38)` contre `danceability (-0.68)`, `valence (-0.62)`, `track_popularity (-0.37)`, `speechiness (-0.39)`. Cette composante principale oppose deux profils de morceaux :\n",
    "    - D’un côté, les morceaux **instrumentaux, longs et peu populaires** (forte contribution de `instrumentalness` et `duration_s`), souvent associés à des genres comme le classique ou le jazz.\n",
    "    - De l’autre, les chansons **courtes, dansantes, joyeuses et populaires** (forte contribution de `danceability`, `valence` et `track_popularity`), typiques de la pop ou de la musique de club. \n",
    "    - Enfin, cette opposition suggère que les morceaux avec des paroles marquées (`speechiness`) et une structure rythmique engageante (`danceability`) sont plus susceptibles de générer de la popularité.\n",
    "- **Composantes principales 2 et 3** : Sur le graphique au centre, on peut extraire plusieurs informations :\n",
    "    - Les morceaux dansants et joyeux (haute valence) s'opposent dans une moindre mesure aux morceaux de faible tempo. De plus, ces morceaux semblent avoir peu de versions live.\n",
    "    - On observe aussi que les morceaux instrumentaux et longs ont généralement une faible popularité, tandis que les morceaux courts et peu instrumentaux sont souvent plus populaires, ce qui est typique de la musique pop, qui est souvent plus accessible et commerciale.\n",
    "- **Composante principale 3** : La troisième composante (CP3) révèle un paradoxe : elle regroupe des morceaux à fort potentiel dansant (`danceability`) et mood positif (`valence`), mais qui restent peu populaires (`track_popularity`). Ces morceaux sont souvent instrumentaux (`instrumentalness`), longs (`duration_s`) et à tempo faible (`tempo`), ce qui les éloigne des standards des charts. Cette composante pourrait représenter des créations artistiques équilibrant danse et complexité, mais peinant à atteindre un large public.\n",
    "\n",
    "Pour mieux comprendre à quoi correspond les type morceaux extraits par ces composantes principales, nous allons regarder les morceaux (individus) contribuant le plus à chacune des composantes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display la mediane de notre dataset quantitatif à fins de comparaison\n",
    "median_values = data_quanti.median()\n",
    "median_values = median_values.to_frame(name='median')\n",
    "median_values = median_values.reset_index()\n",
    "median_values.columns = ['feature', 'median']\n",
    "median_values['feature'] = median_values['feature'].astype('category')\n",
    "median_values = median_values.set_index('feature')\n",
    "median_values = median_values.sort_index()\n",
    "median_values = median_values.T\n",
    "median_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Composante principale 1\n",
    "\n",
    "Afin de mieux comprendre les profils musicaux mis en évidence par la première composante principale, nous allons examiner les morceaux qui y contribuent le plus fortement, positivement et négativement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the contributions of individuals to the first principal component\n",
    "contributions = projected[:, 0]\n",
    "\n",
    "# Get the indices of the top 5 positive contributors\n",
    "top_5_positive_indices = np.argsort(contributions)[-5:]\n",
    "\n",
    "# Get the indices of the top 5 negative contributors\n",
    "top_5_negative_indices = np.argsort(contributions)[:5]\n",
    "\n",
    "# Extract the corresponding rows from the original dataset for positive contributions\n",
    "top_5_positive_tracks = data_quanti.iloc[top_5_positive_indices].copy()\n",
    "top_5_positive_tracks['track_name'] = data.iloc[top_5_positive_indices]['track_name'].values\n",
    "top_5_positive_tracks['track_artist'] = data.iloc[top_5_positive_indices]['track_artist'].values\n",
    "top_5_positive_tracks['contribution'] = contributions[top_5_positive_indices]\n",
    "top_5_positive_tracks['playlist_subgenre'] = data.iloc[top_5_positive_indices]['playlist_subgenre'].values\n",
    "# sort by contribution\n",
    "top_5_positive_tracks = top_5_positive_tracks.sort_values(by='contribution', ascending=False)\n",
    "\n",
    "# Extract the corresponding rows from the original dataset for negative contributions\n",
    "top_5_negative_tracks = data_quanti.iloc[top_5_negative_indices].copy()\n",
    "top_5_negative_tracks['track_name'] = data.iloc[top_5_negative_indices]['track_name'].values\n",
    "top_5_negative_tracks['track_artist'] = data.iloc[top_5_negative_indices]['track_artist'].values\n",
    "top_5_negative_tracks['contribution'] = contributions[top_5_negative_indices]\n",
    "top_5_negative_tracks['playlist_subgenre'] = data.iloc[top_5_negative_indices]['playlist_subgenre'].values\n",
    "\n",
    "# Display the top 5 positive and negative tracks with additional information\n",
    "print(\"Top 5 Positive Contributions (PC1):\")\n",
    "display(top_5_positive_tracks)\n",
    "\n",
    "print(\"Top 5 Negative Contributions (PC1) :\")\n",
    "display(top_5_negative_tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour mieux cerner les types de morceaux représentés aux extrémités de la **première composante principale (PC1)**, nous avons identifié les individus (chansons) ayant les **contributions les plus élevées**, positives comme négatives.\n",
    "\n",
    "- **Du côté des contributions positives**, on retrouve majoritairement des morceaux **rock, hard rock ou pop rock** très énergiques et puissants tels que *American Idiot* (Green Day), *Beauty Queen* (BLVK SWVN) ou *ATTENTION ATTENTION* (Shinedown). Ces morceaux sont caractérisés par une **énergie élevée**, une **forte intensité sonore (loudness)** et une **faible acoustique**, ce qui confirme bien la structure mise en évidence par la CP1. Notons aussi *This Is How We Do It* (Montell Jordan), un morceau R&B énergique, qui se distingue des autres par son genre mais partage les mêmes caractéristiques acoustiques.\n",
    "\n",
    "- **À l’opposé**, les morceaux à contribution très négative sur PC1 sont des titres à **forte acoustique**, **peu énergiques** et **très faibles en loudness**. Il s'agit notamment de sons **ambiants, relaxants ou naturels**, comme *Peaceful Forest* ou *Tropical Rainforest at Dawn*, mais aussi de titres R&B ou indie très doux (*Small* de chloe moriondo). Ces morceaux incarnent l'autre extrémité de la CP1 : **des chansons calmes, acoustiques et à faible énergie**, souvent issues de sous-genres comme *tropical*, *indie poptimism* ou *new jack swing*.\n",
    "\n",
    "Cette opposition renforce l’interprétation de la **CP1 comme un axe énergie / intensité sonore vs. calme / acoustique**, pertinent pour distinguer deux grandes familles de styles musicaux dans le dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Composante principale 2\n",
    "\n",
    "Pour approfondir l’interprétation de la **deuxième composante principale**, nous allons analyser les morceaux qui y contribuent le plus fortement — positivement comme négativement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the contributions of individuals to the second principal component\n",
    "contributions_pc2 = projected[:, 1]\n",
    "\n",
    "# Get the indices of the top 5 positive contributors for PC2\n",
    "top_5_positive_indices_pc2 = np.argsort(contributions_pc2)[-5:]\n",
    "\n",
    "# Get the indices of the top 5 negative contributors for PC2\n",
    "top_5_negative_indices_pc2 = np.argsort(contributions_pc2)[:5]\n",
    "\n",
    "# Extract the corresponding rows from the original dataset for positive contributions (PC2)\n",
    "top_5_positive_tracks_pc2 = data_quanti.iloc[top_5_positive_indices_pc2].copy()\n",
    "top_5_positive_tracks_pc2['track_name'] = data.iloc[top_5_positive_indices_pc2]['track_name'].values\n",
    "top_5_positive_tracks_pc2['track_artist'] = data.iloc[top_5_positive_indices_pc2]['track_artist'].values\n",
    "top_5_positive_tracks_pc2['contribution'] = contributions_pc2[top_5_positive_indices_pc2]\n",
    "top_5_positive_tracks_pc2['playlist_subgenre'] = data.iloc[top_5_positive_indices_pc2]['playlist_subgenre'].values\n",
    "# Sort by contribution\n",
    "top_5_positive_tracks_pc2 = top_5_positive_tracks_pc2.sort_values(by='contribution', ascending=False)\n",
    "\n",
    "# Extract the corresponding rows from the original dataset for negative contributions (PC2)\n",
    "top_5_negative_tracks_pc2 = data_quanti.iloc[top_5_negative_indices_pc2].copy()\n",
    "top_5_negative_tracks_pc2['track_name'] = data.iloc[top_5_negative_indices_pc2]['track_name'].values\n",
    "top_5_negative_tracks_pc2['track_artist'] = data.iloc[top_5_negative_indices_pc2]['track_artist'].values\n",
    "top_5_negative_tracks_pc2['contribution'] = contributions_pc2[top_5_negative_indices_pc2]\n",
    "top_5_negative_tracks_pc2['playlist_subgenre'] = data.iloc[top_5_negative_indices_pc2]['playlist_subgenre'].values\n",
    "\n",
    "# Display the top 5 positive and negative tracks with additional information for PC2\n",
    "print(\"Top 5 Positive Contributions (PC2):\")\n",
    "display(top_5_positive_tracks_pc2)\n",
    "\n",
    "print(\"Top 5 Negative Contributions (PC2):\")\n",
    "display(top_5_negative_tracks_pc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Côté contributions positives**, on retrouve des titres principalement **rap et latino**, tels que *Suge* de DaBaby ou *LAX* de B0nds. Ces morceaux sont :\n",
    "- **courts**,\n",
    "- **dansants** (haute `danceability`),\n",
    "- avec une **valence élevée** (émotion positive),\n",
    "- mais également avec un certain niveau de **speechiness**, notamment pour les titres rap.\n",
    "\n",
    "Ces morceaux partagent donc des caractéristiques propres aux chansons **énergétiques, rythmées et populaires**, souvent taillées pour le streaming, avec des formats courts, accrocheurs et directs.\n",
    "\n",
    "**À l’opposé**, les morceaux ayant une **forte contribution négative à PC2** sont très différents : on retrouve des **paysages sonores naturels, ambiants ou instrumentaux** comme *Rain Forest and Tropical Beach Sound*, *Caribbean Thunderstorm*, ou encore *Battlement*. Ces titres sont :\n",
    "- **longs**,\n",
    "- **instrumentaux** (forte `instrumentalness`),\n",
    "- avec une **faible valence** et **peu de parole**,\n",
    "- et souvent issus de sous-genres comme *tropical*, *album rock*, ou *ambient*.\n",
    "\n",
    "Cela confirme l’interprétation initiale de la PC2 comme un **axe opposant la musique instrumentale, longue et contemplative** à une musique **populaire, dansante et rythmée**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Composante principale 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the contributions of individuals to the third principal component\n",
    "contributions_pc3 = projected[:, 2]\n",
    "\n",
    "# Get the indices of the top 5 positive contributors for PC3\n",
    "top_5_positive_indices_pc3 = np.argsort(contributions_pc3)[-5:]\n",
    "\n",
    "# Get the indices of the top 5 negative contributors for PC3\n",
    "top_5_negative_indices_pc3 = np.argsort(contributions_pc3)[:5]\n",
    "\n",
    "# Extract the corresponding rows from the original dataset for positive contributions (PC3)\n",
    "top_5_positive_tracks_pc3 = data_quanti.iloc[top_5_positive_indices_pc3].copy()\n",
    "top_5_positive_tracks_pc3['track_name'] = data.iloc[top_5_positive_indices_pc3]['track_name'].values\n",
    "top_5_positive_tracks_pc3['track_artist'] = data.iloc[top_5_positive_indices_pc3]['track_artist'].values\n",
    "top_5_positive_tracks_pc3['contribution'] = contributions_pc3[top_5_positive_indices_pc3]\n",
    "top_5_positive_tracks_pc3['playlist_subgenre'] = data.iloc[top_5_positive_indices_pc3]['playlist_subgenre'].values\n",
    "# Sort by contribution\n",
    "top_5_positive_tracks_pc3 = top_5_positive_tracks_pc3.sort_values(by='contribution', ascending=False)\n",
    "\n",
    "# Extract the corresponding rows from the original dataset for negative contributions (PC3)\n",
    "top_5_negative_tracks_pc3 = data_quanti.iloc[top_5_negative_indices_pc3].copy()\n",
    "top_5_negative_tracks_pc3['track_name'] = data.iloc[top_5_negative_indices_pc3]['track_name'].values\n",
    "top_5_negative_tracks_pc3['track_artist'] = data.iloc[top_5_negative_indices_pc3]['track_artist'].values\n",
    "top_5_negative_tracks_pc3['contribution'] = contributions_pc3[top_5_negative_indices_pc3]\n",
    "top_5_negative_tracks_pc3['playlist_subgenre'] = data.iloc[top_5_negative_indices_pc3]['playlist_subgenre'].values\n",
    "\n",
    "# Display the top 5 positive and negative tracks with additional information for PC3\n",
    "print(\"Top 5 Positive Contributions (PC3):\")\n",
    "display(top_5_positive_tracks_pc3)\n",
    "\n",
    "print(\"Top 5 Negative Contributions (PC3):\")\n",
    "display(top_5_negative_tracks_pc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **troisième composante principale** met en lumière une tension plus subtile entre deux types de morceaux aux caractéristiques inattendues.\n",
    "\n",
    "**Du côté des contributions positives**, on retrouve des titres de **pop et R&B calmes et acoustiques** comme *raindrops (an angel cried)* (Ariana Grande) ou *You Are The Reason* (Calum Scott). Ces chansons ont :\n",
    "- une **acousticness élevée** (acapela, guitare acoustique, piano),\n",
    "- un **tempo légèrement plus rapide que la médiane des morceaux**,\n",
    "- une **faible énergie**, mais un **potentiel émotionnel fort** (`valence` variable).\n",
    "\n",
    "Comme suggéré par la PC3, ces morceaux rencontrent un certain succès, illustrant un profil de chansons émotionnelles, accessibles et bien produites, souvent interprétées par des artistes grand public au style sobre et expressif.\n",
    "\n",
    "**Les contributions négatives**, quant à elles, sont largement dominées par des morceaux **EDM ou latino instrumentaux**, comme *I Feel Love* ou *Chase*. Ces morceaux sont :\n",
    "- **longs**,\n",
    "- très **instrumentaux**,\n",
    "- **énergiques** mais souvent **moins \"accessibles\" émotionnellement** (valence très élevée mais peu de paroles, structure répétitive).\n",
    "\n",
    "Ils présentent également une popularité extrêmement faible, allant de 0 à 8. Ces productions s’adressent probablement à un public averti ou sont conçues pour des usages spécifiques (DJ sets, ambiances electro), ce qui les éloigne des standards de la musique grand public.\n",
    "\n",
    "Ces observations confirment que la **PC3 oppose des créations acoustiques à forte charge émotionnelle** à des morceaux **instrumentaux, électroniques, longs**, aux dynamiques parfois complexes ou répétitives.\n",
    "\n",
    "On remarque que le signe des contribution est inversé par rapport à la PCA réalisé en R uniquement pour cette dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\n",
    "\n",
    "df_data_scaled = pd.DataFrame(data_scaled, columns=data_quanti.columns)\n",
    "# set the same index as data_quanti\n",
    "df_data_scaled.index = data_quanti.index\n",
    "\n",
    "# Limit to 500 songs randomly selected to avoid memory crash\n",
    "data_scaled_sample = df_data_scaled.sample(n=500, random_state=1)\n",
    "\n",
    "# Calculate the Manhattan distance matrix\n",
    "manhattan_dist_matrix = manhattan_distances(data_scaled_sample)\n",
    "# Calculate the Euclidean distance matrix\n",
    "euclidean_dist_matrix = euclidean_distances(data_scaled_sample)\n",
    "\n",
    "# Perform MDS using Manhattan distance\n",
    "mds_manhattan = MDS(n_components=2, dissimilarity='precomputed', random_state=1)\n",
    "mds_manhattan_results = mds_manhattan.fit_transform(manhattan_dist_matrix)\n",
    "# Perform MDS using Euclidean distance\n",
    "mds_euclidean = MDS(n_components=2, dissimilarity='precomputed', random_state=1)\n",
    "mds_euclidean_results = mds_euclidean.fit_transform(euclidean_dist_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the MDS results for Manhattan distance with sns\n",
    "# put playlist_genre in the color palette\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.scatterplot(\n",
    "    x=mds_manhattan_results[:, 0],\n",
    "    y=mds_manhattan_results[:, 1],\n",
    "    hue=data_scaled_sample.index,\n",
    "    palette='Set2',\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('MDS with Manhattan Distance', fontsize=20)\n",
    "plt.xlabel('MDS Dimension 1', fontsize=16)\n",
    "plt.ylabel('MDS Dimension 2', fontsize=16)\n",
    "plt.legend(title=qualisup, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# plot the MDS results for Euclidean distance with sns\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.scatterplot(\n",
    "    x=mds_euclidean_results[:, 0],\n",
    "    y=mds_euclidean_results[:, 1],\n",
    "    hue=data_scaled_sample.index,\n",
    "    palette='Set2',\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('MDS with Euclidean Distance', fontsize=20)\n",
    "plt.xlabel('MDS Dimension 1', fontsize=16)\n",
    "plt.ylabel('MDS Dimension 2', fontsize=16)\n",
    "plt.legend(title=qualisup, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform several MDS sns.scatterplot with different hues, for the 11 quantitative variables in data_scaled\n",
    "quantitative_vars = data_scaled_sample.columns\n",
    "\n",
    "# Create a list of colors for the scatterplots\n",
    "colors = sns.color_palette(\"Set2\", len(quantitative_vars), as_cmap=True)\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(4, 3, figsize=(20, 20))\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "# Loop through each quantitative variable and create a scatterplot\n",
    "for i, var in enumerate(quantitative_vars):\n",
    "    sns.scatterplot(\n",
    "        x=mds_euclidean_results[:, 0],\n",
    "        y=mds_euclidean_results[:, 1],\n",
    "        hue=data_scaled_sample[var],\n",
    "        ax=axes[i]\n",
    "    )\n",
    "    axes[i].set_title(f'MDS with {var}', fontsize=16)\n",
    "    axes[i].set_xlabel('MDS Dimension 1', fontsize=14)\n",
    "    axes[i].set_ylabel('MDS Dimension 2', fontsize=14)\n",
    "    axes[i].legend(title=var, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[i].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation :**\n",
    "\n",
    "L’analyse en **MDS (Multidimensional Scaling)** appliquée aux données quantitatives donne des résultats contrastés.\n",
    "\n",
    "* Lorsqu’on projette les points selon une **distance euclidienne ou manhattan**, et qu’on colore les morceaux selon le **genre de playlist**, **aucune structure claire n’émerge** : les genres sont **entièrement mélangés**. Cela montre que, globalement, **les variables quantitatives ne permettent pas de séparer les genres musicaux** dans une représentation MDS. La MDS **ne capture donc pas d'information utile liée au genre musical global** dans cet espace.\n",
    "\n",
    "* Cependant, certaines **variables quantitatives spécifiques sont bien séparées** dans l’espace MDS :\n",
    "\n",
    "  * C’est le cas de **`energy`**, **`loudness`**, **`speechiness`**, **`acousticness`**, **`instrumentalness`**, et dans une moindre mesure **`valence`**.\n",
    "\n",
    "    * Pour **`valence`**, bien que l’on observe deux groupes principaux (valence haute vs. basse), **certains morceaux très joyeux se retrouvent mélangés avec des morceaux tristes**, ce qui rend la séparation moins nette que pour les autres variables.\n",
    "  * On observe des **relations fortes et interprétables** entre certaines variables :\n",
    "\n",
    "    * Sur la **dimension 1 de la MDS**, **`energy` et `loudness` sont corrélées positivement**, et **`acousticness` leur est exactement opposée**. Cela signifie que **les morceaux énergétiques et forts sont peu acoustiques**, ce qui est **parfaitement cohérent avec l’intuition musicale**.\n",
    "    * Sur la **dimension 2**, on retrouve une opposition logique entre **`instrumentalness` et `speechiness`** : **les morceaux très instrumentaux sont peu parlés**, ce qui reflète bien la réalité musicale (ex : classique vs. rap).\n",
    "\n",
    "* En revanche, pour **`track_popularity`**, **`danceability`**, **`tempo`** et **`duration_s`**, **aucune séparation nette n’apparaît** dans l’espace MDS. Ces variables semblent **ne pas être linéairement séparables** par la MDS. On peut donc conclure que **la MDS ne permet pas d’identifier de structure claire** pour ces dimensions — possiblement en raison de leur **relation non linéaire** ou d’un **bruit trop important**.\n",
    "\n",
    "---\n",
    "\n",
    "**Lien avec l’analyse PCA** :\n",
    "\n",
    "On retrouve ici des observations **cohérentes avec l’analyse des composantes principales**. Par exemple :\n",
    "\n",
    "* La **CP1** opposait déjà **`energy` et `loudness`** à **`acousticness`**, exactement comme la **dimension 1 de la MDS**.\n",
    "* La **CP2** révélait une tension entre **`instrumentalness`** et **`speechiness`**, que la **MDS dimension 2** reflète également.\n",
    "* Enfin, comme en PCA, **`track_popularity`**, **`tempo`** et **`duration_s`** ne permettent pas de bien structurer l’espace : ces variables n’étaient déjà que faiblement corrélées avec les premières composantes principales, et la MDS confirme leur faible capacité à structurer les données dans un espace réduit.\n",
    "\n",
    "---\n",
    "\n",
    "Nous allons desormais réaliser un **t-SNE** (t-distributed Stochastic Neighbor Embedding) pour visualiser en 2D la proximité (ou la similarité) entre les chansons Spotify selon leurs caractéristiques audio (danceability, energy, loudness, etc.), pour voir **si des genres musicaux distincts émergent sous forme de groupes**. A l'inverse de la PCA et de la MDS, le **t-SNE est une méthode non linéaire** qui pourrait capturer des relations différentes entre les données que celles révélées par les méthodes linéaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Perform t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=1)\n",
    "tsne_results = tsne.fit_transform(data_scaled_sample)\n",
    "\n",
    "# plot the t-SNE results with sns\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(\n",
    "    x=tsne_results[:, 0],\n",
    "    y=tsne_results[:, 1],\n",
    "    hue=data_scaled_sample.index,\n",
    "    palette='Set2',\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('t-SNE', fontsize=20)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=16)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=16)\n",
    "plt.legend(title=qualisup, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform t-SNE with different hues, for the 11 quantitative variables in data_scaled\n",
    "# Create a list of colors for the scatterplots\n",
    "colors = sns.color_palette(\"Set2\", len(quantitative_vars), as_cmap=True)\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(4, 3, figsize=(20, 20))\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "# Loop through each quantitative variable and create a scatterplot\n",
    "for i, var in enumerate(quantitative_vars):\n",
    "    sns.scatterplot(\n",
    "        x=tsne_results[:, 0],\n",
    "        y=tsne_results[:, 1],\n",
    "        hue=data_scaled_sample[var],\n",
    "        ax=axes[i]\n",
    "    )\n",
    "    axes[i].set_title(f't-SNE with {var}', fontsize=16)\n",
    "    axes[i].set_xlabel('t-SNE Dimension 1', fontsize=14)\n",
    "    axes[i].set_ylabel('t-SNE Dimension 2', fontsize=14)\n",
    "    axes[i].legend(title=var, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[i].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici une version mise en forme dans le même style, avec les éléments importants mis en gras et les liens logiques préservés :\n",
    "\n",
    "---\n",
    "\n",
    "**Interprétation :**\n",
    "\n",
    "Malgré l’application du **t-SNE**, on observe **toujours aussi peu de séparation nette entre les genres musicaux** : les morceaux restent largement **mélangés indépendamment de leur genre**, ce qui confirme les limites des caractéristiques audio pour discriminer les styles musicaux au sens large.\n",
    "\n",
    "Cependant, le **t-SNE révèle une structure beaucoup plus claire que la MDS**, avec l’émergence de **groupes de morceaux** qui semblent s’organiser selon leurs **propriétés audio internes**.\n",
    "\n",
    "* Les variables **`energy`** et **`loudness`** sont **clairement séparées**, avec une **distribution similaire**, ce qui rejoint parfaitement les résultats observés en **PCA (CP1)** et en **MDS (dim 1)**. De même, **`acousticness`** est **opposée** à ces deux variables, avec un groupe bien distinct de **musiques très acoustiques**, caractérisées par **une faible énergie et un faible volume sonore**.\n",
    "\n",
    "* Concernant **`instrumentalness`**, le t-SNE apporte une **information nouvelle importante** : on distingue **deux groupes de morceaux instrumentaux** très différents :\n",
    "\n",
    "  * Un groupe de morceaux **instrumentaux, calmes, acoustiques et peu louds**.\n",
    "  * Un autre groupe **instrumental mais très énergétique, fort en loudness**, donc probablement plus proche de l’expérimental, du post-rock ou de l’électro instrumental.\n",
    "    Cette séparation fine n’apparaissait **ni en PCA, ni en MDS**, et montre que le **t-SNE capture mieux les structures non linéaires** des données.\n",
    "\n",
    "* Pour **`valence`**, on observe **plusieurs sous-groupes assez distincts**, mais **difficiles à interpréter directement**. Toutefois, en croisant avec **`acousticness`**, on remarque que **les morceaux acoustiques et calmes ont généralement une valence très faible**, ce qui correspond à des ambiances tristes ou introspectives.\n",
    "\n",
    "* En revanche, pour des variables comme **`track_popularity`**, **`danceability`**, **`tempo`** ou **`duration_s`**, la séparation reste **floue et peu informative**, ce qui suggère que ces caractéristiques sont **moins déterminantes** pour structurer l’espace audio selon t-SNE.\n",
    "\n",
    "---\n",
    "\n",
    "**Bilan sur les méthodes de réduction de dimension quantitatives :**\n",
    "\n",
    "* La **PCA** fournit une **lecture linéaire et interprétable** des relations entre variables, et met en évidence des axes clairs comme l’opposition **énergie vs. acoustique** ou **instrumental vs. populaire**.\n",
    "* La **MDS**, malgré son approche plus flexible sur les distances, **n’apporte pas d’information supplémentaire significative** par rapport à la PCA, sauf sur des dimensions très spécifiques.\n",
    "* Le **t-SNE**, quant à lui, **révèle des regroupements non linéaires** que les autres méthodes ne captent pas, et permet de **mieux explorer des structures complexes**, notamment pour des variables comme **`instrumentalness`**, **`energy`**, et **`acousticness`**.\n",
    "\n",
    "En conclusion, chaque méthode a ses forces : **la PCA pour l’interprétation globale des variables**, **le t-SNE pour l'exploration fine des regroupements**, et **la MDS reste plus limitée** dans ce cas d’étude.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Factorial Analysis (MCA)\n",
    "\n",
    "**1. Variables catégorielles à utiliser dans la MCA**\n",
    "Voici les variables qualitatives que l'on pourrait envisager d'utiliser pour la MCA sur le dataset Spotify :  \n",
    "- `track_artist`  \n",
    "- `track_album_name` (attention à trop de modalités rares, peut-être garder que les plus fréquentes ou ne pas inclure à cause du grand nombre d’artistes)  \n",
    "- `playlist_name`  \n",
    "- `playlist_genre`  \n",
    "- `playlist_subgenre`  \n",
    "- `key`  \n",
    "- `mode`  \n",
    "\n",
    "---\n",
    "\n",
    "**2. Questions que la MCA peut aider à explorer**\n",
    "\n",
    "**a) Quels sont les groupes/cluster de modalités similaires ?**\n",
    "- Est-ce que certains genres et sous-genres de playlists s’associent fréquemment ?  \n",
    "- Certains \"modes\" (majeur/minor) sont-ils plus fréquents dans certains genres ?  \n",
    "- Y a-t-il des clés (`key`) musicales qui sont typiques de certains genres ou playlists ?  \n",
    "\n",
    "La MCA permettra de représenter graphiquement (biplot) ces modalités et d’identifier des associations fortes.\n",
    "\n",
    "**b) Est-ce que certains artistes ou playlists ont un profil qualitatif particulier ?**\n",
    "- Par exemple, certains artistes seraient-ils associés à un genre et sous-genre spécifiques, ou à un mode particulier ?  \n",
    "- Y a-t-il des clusters d’artistes / playlists qui partagent des caractéristiques particulières (clé, mode, genre) ?\n",
    "\n",
    "**3. Comment interpréter la MCA ici**\n",
    "\n",
    "- **Axes factoriels** : Chaque axe correspond à une dimension qui résume des associations fortes entre modalités. Par exemple, un axe peut opposer les genres \"Rock\" à \"Pop\", ou des clés majeures à mineures.\n",
    "- **Modalités proches dans l’espace** : Modalités proches signifient qu’elles co-apparaissent souvent dans les observations (ex. certains genres + mode majeur).\n",
    "- **Observation** : Si tu represents les observations (chansons) dans l’espace MCA, celles proches partagent des profils catégoriels similaires.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Utilisations concrètes/academic use cases**\n",
    "\n",
    "- **Profil des genres musicaux** : Comprendre quels modes/clés/sous-genres caractérisent les genres populaires sur Spotify.  \n",
    "- **Segmentation qualitative des playlists** : Y a-t-il des types de playlists/musiques qui partagent des caractéristiques qualitatives communes ?  \n",
    "- **Analyse de diversité** : Mesurer dans quelle mesure certains artistes/genres sont hétérogènes ou homogènes quant à leurs caractéristiques catégorielles.  \n",
    "- **Préparation à une classification** : Par exemple, combiner le résultat de la PCA (variables numériques) avec la MCA (variables qualitatives) dans une analyse factorielle mixte ou pour enrichir un modèle prédictif.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Exemple de questions précises à poser**\n",
    "\n",
    "- Les genres musicaux sont-ils liés à certaines tonalités ou modes ?  \n",
    "- Les sous-genres présents dans la même playlist sont-ils proches ou éloignés dans l’espace MCA ?  \n",
    "- Y a-t-il des clés rares ou des modes minoritaires associés à certains genres uniquement ?  \n",
    "- Peut-on détecter des groupes de playlists ou artistes avec des profils qualitatifs similaires ?  \n",
    "\n",
    "---\n",
    "\n",
    "**En conclusion**\n",
    "La MCA t’aide surtout à **explorer et visualiser les relations entre variables qualitatives** et leurs modalités sur ton dataset Spotify, ce qui complète bien la PCA sur les variables numériques. C’est une étape utile pour comprendre la structure qualitative de tes données avant d’envisager une modélisation supervisée ou une analyse plus approfondie.\n",
    "\n",
    "---\n",
    "\n",
    "Si tu veux, je peux aussi te fournir un exemple de code Python pour réaliser une MCA avec `prince` ou en R avec `FactoMineR` sur ton dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import prince \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Subset for MCA\n",
    "df_cat = data[['playlist_genre', 'mode', 'key']].copy()\n",
    "\n",
    "# Run MCA\n",
    "mca = prince.MCA(n_components=2, n_iter=100, copy=True, check_input=True, engine='sklearn', random_state=1)\n",
    "mca = mca.fit(df_cat)\n",
    "\n",
    "# Get coordinates for the plot\n",
    "coords = mca.column_coordinates(df_cat)\n",
    "\n",
    "# Calculate cosine similarities for sizing\n",
    "cosine_similarities = mca.column_cosine_similarities(df_cat)\n",
    "# Use the sum of cosine similarities as size parameter (for clearer visualization)\n",
    "sizes = cosine_similarities[0]**2 + cosine_similarities[1]**2\n",
    "\n",
    "# Prepare data for plotting\n",
    "plot_data = pd.DataFrame({\n",
    "    'x': coords[0],\n",
    "    'y': coords[1],\n",
    "    'variable': [x.split('__')[0] for x in coords.index],\n",
    "    'value': [x.split('__')[1] for x in coords.index],\n",
    "    'size': sizes * 500  # Scale sizes for better visualization\n",
    "})\n",
    "\n",
    "# Create a color mapping for variables\n",
    "colors = {'playlist_genre': 'blue', 'mode': 'red', 'key': 'green'}\n",
    "plot_data['color'] = plot_data['variable'].map(colors)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = sns.scatterplot(\n",
    "    data=plot_data,\n",
    "    x='x', \n",
    "    y='y',\n",
    "    hue='variable',\n",
    "    size='size',\n",
    "    sizes=(50, 500),\n",
    "    palette=colors,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Add annotations\n",
    "for i, row in plot_data.iterrows():\n",
    "    plt.text(\n",
    "        row['x'] + 0.02, \n",
    "        row['y'] + 0.02, \n",
    "        row['value'], \n",
    "        fontsize=9,\n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "# Add styling\n",
    "plt.title('MCA: Relationship between Genre, Mode and Key', fontsize=16)\n",
    "plt.xlabel(f'Dimension 1 ({mca.eigenvalues_summary.iloc[0, 1]})', fontsize=12)\n",
    "plt.ylabel(f'Dimension 2 ({mca.eigenvalues_summary.iloc[1, 1]})', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='gray', linestyle='-', alpha=0.3)\n",
    "plt.legend(title='Variable Type')\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print quick interpretation of coordinates\n",
    "print(\"\\nCoordinates Summary by Variable Type:\")\n",
    "for var in ['playlist_genre', 'mode', 'key']:\n",
    "    var_coords = coords.filter(like=var)\n",
    "    print(f\"\\n{var.upper()} coordinates:\")\n",
    "    print(var_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables utilisées\n",
    "Les variables qualitatives sélectionnées pour cette analyse sont :\n",
    "- `playlist_genre` : Genre de la playlist (ex. rock, pop, rap, etc.).\n",
    "- `key` : Tonalité musicale (ex. C, D, E♭, etc.).\n",
    "- `mode` : Mode musical (majeur ou mineur).\n",
    "\n",
    "### Résultats\n",
    "\n",
    "#### 1. Axes factoriels\n",
    "- **Dim1 (8%)** : Cet axe semble opposer des genres musicaux et des tonalités spécifiques :\n",
    "  - À gauche, des genres comme `rap` et des tonalités comme `A♯/B♭` sont associés à des morceaux modernes ou spécifiques.\n",
    "  - À droite, des tonalités comme `C` et `G` sont associées à des genres comme `rock`, suggérant une relation avec des styles plus classiques.\n",
    "- **Dim2 (6.6%)** : Cet axe reflète une distinction entre les modes (`major` et `minor`) et leur association avec certains genres :\n",
    "  - En bas, le mode `major` est associé à des genres comme `rock`.\n",
    "  - En haut, le mode `minor` est plus proche de genres comme `rap` et `edm`.\n",
    "\n",
    "On remarque ici que nous avons les même resultats qu'en R à l'exception de la projection des individus qui est inversée.\n",
    "\n",
    "#### 2. Proximité des modalités\n",
    "- Les modalités proches sur le graphique sont souvent associées dans les données :\n",
    "  - `pop`, `latin`, `r&b` et `edm` sont centrés par rapport aux modes `minor`et `major`, indiquant qu'ils n'appartiennent pas clairement à un mode spécifique, mais partagent des caractéristiques communes.\n",
    "  - `rap` est également centré par rapport à ces modes, ce qui suggère que l'utilisation des modes majeurs et mineurs est assez équilibrée dans la composition des morceaux de rap. Toutefois, il reste distinct des groupes `pop`, `latin`, `r&b` et `edm`.\n",
    "  - `rock` est plus proche de `major`, ce qui suggère que les musiques de ce genre sont souvent associées à des tonalités majeures.\n",
    "  - Les tonalités comme `B`,`D♯/E♭`,`A♯/B♭`,`F♯/G♭`et `F` sont proches de `minor`, indiquant qu'elles sont souvent utilisées dans des morceaux en mode mineur.\n",
    "  - Les tonalités comme `C`, `G`,`D` sont proches de `major`, ce qui suggère qu'elles sont souvent utilisées dans des morceaux en mode majeur.\n",
    "\n",
    "#### 3. Cos² (Qualité de représentation)\n",
    "- La taille des bulles indiquent la qualité de représentation des modalités sur les deux premières dimensions :\n",
    "  - Les modalités avec des grosses bulles comme `major`, `minor`, `C♯/D♭` ou `rock` sont bien représentées sur ces axes.\n",
    "  - Les modalités avec des bulles plus petites comme certaines tonalités, ou certains genres (`pop`, `latin`, `r&b` et `edm`) sont moins bien représentées, ce qui signifie qu'elles pourraient être mieux expliquées par d'autres dimensions.\n",
    "\n",
    "### Conclusion\n",
    "Cette MCA met en évidence des associations claires entre les genres musicaux, les tonalités (`key`), et les modes (`major`/`minor`). Elle permet de visualiser les relations qualitatives dans les données et d'identifier des clusters ou des oppositions significatives. Par exemple :\n",
    "- `rock` est distinct des autres genres, avec des tonalités et un mode spécifiques.\n",
    "- `pop`, `latin`, `r&b` et `edm` partagent des caractéristiques similaires, mais ne sont pas clairement associés à un mode particulier.\n",
    "- `rap` est centré par rapport aux modes, mais reste distinct des autres genres.\n",
    "\n",
    "Ces résultats offrent une meilleure compréhension des structures qualitatives des données et peuvent être utilisés pour des analyses complémentaires, comme la segmentation ou la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir 4 artistes fréquents\n",
    "top_artists = ['David Guetta', 'Ed Sheeran', 'Green Day', 'Billie Eilish', 'Eminem']\n",
    "\n",
    "# Remplacer les autres artistes par 'Autre'\n",
    "data['artist_subset'] = data['track_artist'].apply(lambda x: x if x in top_artists else 'Autre')\n",
    "\n",
    "# Refaire le sous-ensemble avec artiste\n",
    "df_cat = data[['playlist_genre', 'mode', 'key', 'artist_subset']].copy()\n",
    "\n",
    "# Run MCA\n",
    "mca = prince.MCA(n_components=2, n_iter=100, copy=True, check_input=True, engine='sklearn', random_state=1)\n",
    "mca = mca.fit(df_cat)\n",
    "\n",
    "# Get coordinates for the plot\n",
    "coords = mca.column_coordinates(df_cat)\n",
    "\n",
    "# Calculate cosine similarities for sizing\n",
    "cosine_similarities = mca.column_cosine_similarities(df_cat)\n",
    "# Use the sum of cosine similarities as size parameter (for clearer visualization)\n",
    "sizes = cosine_similarities[0]**2 + cosine_similarities[1]**2\n",
    "\n",
    "# Prepare data for plotting\n",
    "plot_data = pd.DataFrame({\n",
    "    'x': coords[0],\n",
    "    'y': coords[1],\n",
    "    'variable': [x.split('__')[0] for x in coords.index],\n",
    "    'value': [x.split('__')[1] for x in coords.index],\n",
    "    'size': sizes * 500  # Scale sizes for better visualization\n",
    "})\n",
    "\n",
    "# Create a color mapping for variables\n",
    "colors = {'playlist_genre': 'blue', 'mode': 'red', 'key': 'green', 'artist_subset': 'purple'}\n",
    "plot_data['color'] = plot_data['variable'].map(colors)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = sns.scatterplot(\n",
    "    data=plot_data,\n",
    "    x='x', \n",
    "    y='y',\n",
    "    hue='variable',\n",
    "    size='size',\n",
    "    sizes=(50, 500),\n",
    "    palette=colors,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Add annotations\n",
    "for i, row in plot_data.iterrows():\n",
    "    plt.text(\n",
    "        row['x'] + 0.02, \n",
    "        row['y'] + 0.02, \n",
    "        row['value'], \n",
    "        fontsize=9,\n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "# Add styling\n",
    "plt.title('MCA: Relationship between Genre, Mode and Key', fontsize=16)\n",
    "plt.xlabel(f'Dimension 1 ({mca.eigenvalues_summary.iloc[0, 1]})', fontsize=12)\n",
    "plt.ylabel(f'Dimension 2 ({mca.eigenvalues_summary.iloc[1, 1]})', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='gray', linestyle='-', alpha=0.3)\n",
    "plt.legend(title='Variable Type')\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print quick interpretation of coordinates\n",
    "print(\"\\nCoordinates Summary by Variable Type:\")\n",
    "for var in ['playlist_genre', 'mode', 'key', 'artist_subset']:\n",
    "    var_coords = coords.filter(like=var)\n",
    "    print(f\"\\n{var.upper()} coordinates:\")\n",
    "    print(var_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les genres et les artistes\n",
    "genres = [i for i in coords.index if i.startswith(\"playlist_genre__\")]\n",
    "artists = [i for i in coords.index if i.startswith(\"artist_subset__\")]\n",
    "\n",
    "# Initialiser un nouveau DataFrame pour stocker les distances\n",
    "distances = pd.DataFrame(index=artists, columns=genres)\n",
    "\n",
    "# Calculer les distances euclidiennes au carré\n",
    "for artist in artists:\n",
    "    for genre in genres:\n",
    "        vec_artist = coords.loc[artist].values\n",
    "        vec_genre = coords.loc[genre].values\n",
    "        dist_sq = np.sum((vec_artist - vec_genre) ** 2)\n",
    "        distances.loc[artist, genre] = dist_sq\n",
    "\n",
    "# Convertir les distances en float (au cas où)\n",
    "distances = distances.astype(float)\n",
    "\n",
    "# Afficher les résultats\n",
    "distances.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distances artistes-genres (MCA)\n",
    "Nous avons décidé d'affiner notre MCA en y intégrant certains artistes pour mieux comprendre leurs relations avec les genres musicaux. Nous avons sélectionné 6 artistes : **David Guetta**, **Ed Sheeran**, **Green Day**, **Billie Eilish**, **Eminem** et un groupe d'artistes diversifié nommé **Autre**. Afin d'analyser la proximité entre ces artistes et les genres musicaux, nous avons calculé les distances euclidiennes au carré entre chaque artiste et les genres issus de l'Analyse des Correspondances Multiples (MCA).\n",
    "\n",
    "#### Interprétations individuelles\n",
    "\n",
    "- **David Guetta** est très proche du genre **EDM**, ce qui est cohérent avec son positionnement d’artiste électro. Il est aussi relativement proche du genre `pop` et `rock`, ce qui peut s’expliquer par ses nombreuses collaborations avec des chanteurs populaires.\n",
    "\n",
    "- **Ed Sheeran** est modérément proche des genres **pop** et **edm**, ce qui reflète sa diversité musicale et ses morceaux mêlant acoustique et sons plus produits.\n",
    "\n",
    "- **Green Day** est relativement proche du genre **rock**, ce qui confirme son ancrage dans ce style. Il est plus éloigné des autres genres, ce qui le distingue nettement stylistiquement dans cette analyse.\n",
    "\n",
    "- **Billie Eilish** est située à mi-distance de plusieurs genres (`pop`, `edm`, `rock`), ce qui traduit une position hybride. Elle n’est pas fortement associée à un genre unique, ce qui est cohérent avec son style singulier et difficile à catégoriser.\n",
    "\n",
    "- **Eminem** montre une **affinité forte avec le genre Rap**, avec une distance minimale comparée aux autres genres. Il est largement séparé des genres comme rock ou pop, ce qui reflète bien son style musical distinct.\n",
    "\n",
    "- **Autre** est, comme attendu, proche du centre, ce qui traduit une position moyenne, résultant de l’agrégation d’artistes divers aux profils variés.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "L’analyse des distances artistes-genres via la MCA confirme plusieurs intuitions sur les positionnements stylistiques des artistes. Elle permet également de détecter des cas hybrides (comme Billie Eilish)\n",
    "Elle enrichit l’interprétation visuelle de l’ACM en objectivant la notion de \"proximité\" par une mesure quantitative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features = [\n",
    "    'danceability', 'energy', 'loudness', 'speechiness',\n",
    "    'acousticness', 'instrumentalness', 'liveness',\n",
    "    'valence', 'tempo', 'duration_s'\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Sélection des features audio\n",
    "X = data_songs[audio_features].copy()\n",
    "\n",
    "# 2. Normalisation min-max (important pour la NMF)\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Tester plusieurs valeurs de r (nombre de composants)\n",
    "errors = []\n",
    "r_values = range(2,11, 2)\n",
    "\n",
    "for r in r_values:\n",
    "    model = NMF(n_components=int(r), init='nndsvda', random_state=42, max_iter=500)\n",
    "    W = model.fit_transform(X_scaled)\n",
    "    H = model.components_\n",
    "    reconstruction = np.dot(W, H)\n",
    "    error = np.linalg.norm(X_scaled - reconstruction, 'fro')  # norme de Frobenius\n",
    "    errors.append(error)\n",
    "\n",
    "# 4. Tracer la courbe de l’erreur de reconstruction\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(r_values, errors, marker='o')\n",
    "plt.title(\"Erreur de reconstruction vs nombre de composants (r)\")\n",
    "plt.xlabel(\"Nombre de composants (r)\")\n",
    "plt.ylabel(\"Erreur de reconstruction (norme de Frobenius)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On garde 6 profils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Application de la NMF\n",
    "nmf_model = NMF(n_components=6, init='nndsvda', random_state=42, max_iter=500)\n",
    "W = nmf_model.fit_transform(X_scaled)\n",
    "H = nmf_model.components_\n",
    "\n",
    "# 4. Créer un DataFrame pour la matrice H\n",
    "H_df = pd.DataFrame(H, columns=audio_features)\n",
    "H_df.index = [f'Profil {i+1}' for i in range(H.shape[0])]\n",
    "\n",
    "# 5. Affichage de la matrice H\n",
    "print(\"Matrice H (profils latents définis par les variables audio) :\")\n",
    "display(H_df)\n",
    "\n",
    "# 6. Visualisation : contribution des variables à chaque profil\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(H_df, annot=True, cmap='YlGnBu', fmt=\".2f\")\n",
    "plt.title(\"Profils latents musicaux détectés par NMF (matrice H)\")\n",
    "plt.xlabel(\"Caractéristiques audio\")\n",
    "plt.ylabel(\"Profils NMF\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset utilisé\n",
    "Nous avons décidé d'utiliser le dataframe `data_songs` ne contenant que les morceaux une seule fois dans le dataset, pour éviter de biaiser les résultats. \n",
    "\n",
    "### Variables utilisées\n",
    "\n",
    "Les variables sélectionnées pour cette NMF sont les caractéristiques audio continues suivantes :\n",
    "\n",
    "* `danceability`, `energy`, `loudness`, `speechiness`, `acousticness`, `instrumentalness`, `liveness`, `valence`, `tempo`, `duration_s`.\n",
    "\n",
    "Nous avons exclu la variable `track_popularity` car la NMF est très sensible aux valeurs extrêmes et cette variable a une distribution très héterogène. De plus, nous avons exclu les variables catégorielles et les variables de texte, car la NMF est conçue pour fonctionner sur des données numériques continues.\n",
    "\n",
    "### Résultats\n",
    "\n",
    "#### 1. Profils latents détectés\n",
    "\n",
    "L’analyse a révélé **6 profils musicaux latents** à partir des données. Chaque profil est défini par une combinaison unique de caractéristiques audio. Voici leur interprétation :\n",
    "\n",
    "* **Profil 1 : Profil Énergique-Intense**\n",
    "\n",
    "  * Caractérisé par de **très fortes valeurs en `energy`, `loudness` et `tempo` et `duration`**.\n",
    "  * Faible en composantes émotionnelles (`valence`), vocales et instrumentales.\n",
    "  * Ce profil pourrait correspondre à des morceaux **très dynamiques et bruyants**, typiques de l’**EDM** (house, electro, techno) ou du **hard-rock**/**metal**.\n",
    "\n",
    "* **Profil 2 : Profil Acoustique-Chant**\n",
    "\n",
    "  * Dominé par une forte **`acousticness`**.\n",
    "  * Ce profil, presente **`instrumentalness`=0**, et un faible **`speechiness`** mais différent de zero.\n",
    "  * Ce profil regroupe possiblement des morceaux **acoustiques et chantés**.\n",
    "\n",
    "* **Profil 3 : Profil Instrumental**\n",
    "\n",
    "  * Très forte **`instrumentalness`** avec très peu d'autres caractéristiques.\n",
    "  * Correspond clairement à des morceaux **purement instrumentaux**, probablement **ambient**, **classique** ou **bande-son**.\n",
    "\n",
    "* **Profil 4 : Profil Émotionnel-Valence**\n",
    "\n",
    "  * Faible en toutes dimensions sauf une très forte **`valence`**.\n",
    "  * Ce profil regroupe des morceaux **émotionnellement très positifs**, avec une ambiance **joyeuse ou euphorique**, indépendamment du tempo ou de l’énergie.\n",
    "\n",
    "* **Profil 5 : Profil Vocal-Live**\n",
    "\n",
    "  * Faibles valeurs générales sauf une **forte `liveness`** et une **`speechiness`** marquée.\n",
    "  * Peut représenter des morceaux **live**, **rap**, ou des chansons **parlées/spoken word**.\n",
    "\n",
    "* **Profil 6 : Profil Rap-Rythmé**\n",
    "\n",
    "  * Forte `danceability`, `speechiness` et modérée `energy`.\n",
    "  * Ce profil semble capturer des morceaux **rythmés et très vocaux**, typiques du **rap**, **hip-hop**, voire certains morceaux **R\\&B urbains**. Ils pourraient aussi correspondre à des morceaux **pop** et **latino** dans une certaine mesure.\n",
    "\n",
    "---\n",
    "\n",
    "Pour chaque profil, nous allons desormais regarder les morceaux qui y contribuent le plus fortement, pour confirmer ces resultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 2 tracks for each NMF profile\n",
    "n_profiles = 6  # Number of profiles from our NMF model\n",
    "top_n = 2      # Number of top tracks to display per profile\n",
    "\n",
    "# Create a dataframe with track information and profile weights\n",
    "profile_weights = pd.DataFrame(W, index=data_songs.index)\n",
    "profile_weights.columns = [f'Profile_{i+1}' for i in range(n_profiles)]\n",
    "\n",
    "# For each profile, find the tracks with the highest weights\n",
    "results = []\n",
    "\n",
    "for profile_idx in range(n_profiles):\n",
    "    profile_name = f'Profile_{profile_idx+1}'\n",
    "    \n",
    "    # Get top tracks for this profile\n",
    "    top_indices = profile_weights[profile_name].nlargest(top_n).index\n",
    "    \n",
    "    # Extract relevant information for these tracks\n",
    "    for idx in top_indices:\n",
    "        track_info = {\n",
    "            'Profile': profile_name,\n",
    "            'Track Name': data.loc[idx, 'track_name'],\n",
    "            'Artist': data.loc[idx, 'track_artist'],\n",
    "            'Genre': data.loc[idx, 'playlist_genre'],\n",
    "            'Subgenre': data.loc[idx, 'playlist_subgenre'],\n",
    "            'Weight': profile_weights.loc[idx, profile_name]\n",
    "        }\n",
    "        \n",
    "        # Add audio features for context\n",
    "        for feature in audio_features:\n",
    "            track_info[feature] = data.loc[idx, feature]\n",
    "            \n",
    "        results.append(track_info)\n",
    "\n",
    "# Create a dataframe with the results and display it\n",
    "top_tracks_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by profile and weight\n",
    "top_tracks_df = top_tracks_df.sort_values(['Profile', 'Weight'], ascending=[True, False])\n",
    "display(top_tracks_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation :**\n",
    "\n",
    "On retrouve des resultats cohérents avec les profils identifiés précédemment. Seul `Sandstorm` de Darude qui est censé être un morceau EDM est classé dans le profil 2, ce qui est surprenant. Mais en regardant la caractéristique `instrumentalness`, on remarque qu'il est très proche de 1, ce qui signifie que le morceau est classé comme très intrumental. Il est donc normal qu'il soit classé dans le profil 2.\n",
    "\n",
    "---\n",
    "\n",
    "On peut desormais regarder pour certains artistes quels sont les profils musicaux qui leur correspondent le mieux. On va regarder les tracks des artistes suivants : **David Guetta**, **Ed Sheeran**, **Green Day**, **Billie Eilish** et **Eminem**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with profile weights for each track\n",
    "profile_weights = pd.DataFrame(W, index=data_songs.index)\n",
    "profile_weights.columns = [f'Profile_{i+1}' for i in range(n_profiles)]\n",
    "\n",
    "# Merge with original data to get artist info\n",
    "artist_profiles = pd.concat([data_songs[['track_artist', 'track_name']], profile_weights], axis=1)\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "artist_distribution = {}\n",
    "\n",
    "# For each artist, analyze their tracks\n",
    "for artist in top_artists:\n",
    "    # Filter tracks by this artist\n",
    "    artist_tracks = artist_profiles[artist_profiles['track_artist'] == artist]\n",
    "    \n",
    "    if len(artist_tracks) == 0:\n",
    "        print(f\"No tracks found for {artist}\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate average profile weight for this artist\n",
    "    avg_profile_weights = artist_tracks[[f'Profile_{i+1}' for i in range(n_profiles)]].mean()\n",
    "    \n",
    "    # Find dominant profile for each track\n",
    "    dominant_profiles = artist_tracks[[f'Profile_{i+1}' for i in range(n_profiles)]].idxmax(axis=1).value_counts()\n",
    "    \n",
    "    # Store results\n",
    "    artist_distribution[artist] = {\n",
    "        'track_count': len(artist_tracks),\n",
    "        'avg_weights': avg_profile_weights,\n",
    "        'dominant_profiles': dominant_profiles\n",
    "    }\n",
    "\n",
    "# Visualize the results\n",
    "fig, axes = plt.subplots(len(top_artists), 2, figsize=(18, 4*len(top_artists)))\n",
    "\n",
    "for i, artist in enumerate(top_artists):\n",
    "    if artist not in artist_distribution:\n",
    "        continue\n",
    "    \n",
    "    # Average profile weights\n",
    "    artist_distribution[artist]['avg_weights'].plot(\n",
    "        kind='bar', \n",
    "        ax=axes[i, 0], \n",
    "        color='skyblue',\n",
    "        title=f\"{artist}: Average Profile Weights (n={artist_distribution[artist]['track_count']} tracks)\"\n",
    "    )\n",
    "    axes[i, 0].set_ylabel(\"Weight\")\n",
    "    axes[i, 0].set_xlabel(\"NMF Profile\")\n",
    "    \n",
    "    # Distribution of dominant profiles\n",
    "    if len(artist_distribution[artist]['dominant_profiles']) > 0:\n",
    "        artist_distribution[artist]['dominant_profiles'].plot(\n",
    "            kind='bar', \n",
    "            ax=axes[i, 1], \n",
    "            color='coral',\n",
    "            title=f\"{artist}: Dominant Profile Distribution\"\n",
    "        )\n",
    "        axes[i, 1].set_ylabel(\"Number of Tracks\")\n",
    "        axes[i, 1].set_xlabel(\"Dominant NMF Profile\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print a summary\n",
    "print(\"\\nArtist Profile Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for artist in top_artists:\n",
    "    if artist not in artist_distribution:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{artist} ({artist_distribution[artist]['track_count']} tracks)\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Get the dominant profile(s)\n",
    "    if len(artist_distribution[artist]['dominant_profiles']) > 0:\n",
    "        top_profile = artist_distribution[artist]['dominant_profiles'].index[0]\n",
    "        top_count = artist_distribution[artist]['dominant_profiles'].iloc[0]\n",
    "        top_percent = (top_count / artist_distribution[artist]['track_count']) * 100\n",
    "        \n",
    "        print(f\"Primary Profile: {top_profile} ({top_percent:.1f}% of tracks)\")\n",
    "    \n",
    "    # Get the highest weight profile on average\n",
    "    top_avg_profile = artist_distribution[artist]['avg_weights'].idxmax()\n",
    "    top_avg_weight = artist_distribution[artist]['avg_weights'].max()\n",
    "    \n",
    "    print(f\"Highest Average Weight: {top_avg_profile} (weight: {top_avg_weight:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tracks by Green Day\n",
    "green_day_tracks = data_songs[data_songs['track_artist'] == 'Green Day']\n",
    "\n",
    "# Create a dataframe with profile weights for each Green Day track\n",
    "profile_weights = pd.DataFrame(W, index=data_songs.index)\n",
    "profile_weights.columns = [f'Profile_{i+1}' for i in range(n_profiles)]\n",
    "\n",
    "# Merge track information with profile weights\n",
    "green_day_profiles = pd.concat([green_day_tracks[['track_name']], \n",
    "                               profile_weights.loc[green_day_tracks.index]], axis=1)\n",
    "\n",
    "# Add the dominant profile column\n",
    "green_day_profiles['Dominant_Profile'] = green_day_profiles[[f'Profile_{i+1}' for i in range(n_profiles)]].idxmax(axis=1)\n",
    "\n",
    "# Sort by dominant profile and then by the weight of that profile (descending)\n",
    "green_day_profiles = green_day_profiles.sort_values(['Dominant_Profile', 'track_name'])\n",
    "\n",
    "# Display the tracks with their profile information\n",
    "display(green_day_profiles[['track_name', 'Dominant_Profile'] + \n",
    "                          [f'Profile_{i+1}' for i in range(n_profiles)]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artistes et profils musicaux\n",
    "\n",
    "En dehors de **Green Day**, ayant des morceaux partagés entre les profiles 4, 5 et 6, **les autres artistes appartiennent tous au profil 6**. Pour ce qui est de **Green Day**, les morceaux qui lui sont attribués dans le profil 4 sont en effet des morceaux joyeux et gais comme *American Idiot* ou *Basket Case* du moins dans la direction artistique.\n",
    "\n",
    "Les morceaux qui lui sont attribués dans le **profil 5** sont des morceaux **qui a priori ne sont pas des versions live** mais l'algorithme de Spotify en charge de detecter les versions live **a du se tromper**. Si cette variable présente des valeurs non représentatives, il serait intéressante de répéter l'analyse sans cette variable.\n",
    "\n",
    "Pour ce qui est des morceaux qui lui sont attribués dans le **profil 6**, ce sont des morceaux pop-rock **plutot équilibrés** comme *Boulevard of Broken Dreams* ou *Wake Me Up When September Ends*.\n",
    "\n",
    "---\n",
    "\n",
    "On peut conclure sur le fait que ce profil 6 regroupe plus de styles musicaux que les autres profils. Il semble être **le profil le plus généraliste**, et regroupe des morceaux de plusieurs genres musicaux.\n",
    "\n",
    "Il serait utile de faire un clustering sur les profils musicaux pour voir si on peut regrouper les morceaux en fonction de leurs caractéristiques audio, particulièrement sur le profil 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib import colors\n",
    "from yellowbrick.cluster import SilhouetteVisualizer, KElbowVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(init='k-means++', n_init='auto', random_state=42)\n",
    "visualizer = KElbowVisualizer(kmeans, k=(2, 12))\n",
    "visualizer.fit(W)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering on W with 3 clusters\n",
    "kmeans_w = KMeans(n_clusters=4, random_state=42, n_init='auto')\n",
    "cluster_labels = kmeans_w.fit_predict(W)\n",
    "\n",
    "# Add cluster labels to a DataFrame for easier handling\n",
    "clustered_data = data_songs.copy()\n",
    "clustered_data['kmeans_cluster'] = cluster_labels\n",
    "\n",
    "# 1. Confusion Matrix: K-Means clusters vs. NMF profiles\n",
    "\n",
    "# Determine the dominant NMF profile for each track\n",
    "# profile_weights was calculated in CELL INDEX 50 and has data_songs.index\n",
    "dominant_nmf_profile = profile_weights.idxmax(axis=1)\n",
    "clustered_data['dominant_nmf_profile'] = dominant_nmf_profile\n",
    "\n",
    "# Create confusion matrix\n",
    "# Ensure the labels match the unique values in dominant_nmf_profile and kmeans_cluster\n",
    "nmf_labels = clustered_data['dominant_nmf_profile'].unique()\n",
    "kmeans_labels = sorted(clustered_data['kmeans_cluster'].unique())\n",
    "\n",
    "# Create a contingency matrix instead of a confusion matrix\n",
    "contingency_matrix = pd.crosstab(clustered_data['dominant_nmf_profile'], clustered_data['kmeans_cluster'])\n",
    "\n",
    "# Convert the contingency matrix to a DataFrame for visualization\n",
    "cm_nmf_df = contingency_matrix\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_nmf_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix: K-Means Clusters vs. Dominant NMF Profiles')\n",
    "plt.ylabel('Dominant NMF Profile')\n",
    "plt.xlabel('K-Means Cluster')\n",
    "plt.show()\n",
    "\n",
    "# 2. Confusion Matrix: K-Means clusters vs. Playlist Genres\n",
    "\n",
    "# Get playlist genres for tracks in data_songs from the original 'data' DataFrame\n",
    "# The 'data' DataFrame (CELL INDEX 2, 5, 6, 40) has 'playlist_genre' and is indexed appropriately\n",
    "# to be used with data_songs.index if data_songs was derived from data.\n",
    "# data_songs was created from data (CELL INDEX 10), so their indices might not align directly if data_songs was re-indexed.\n",
    "# However, W is indexed by data_songs.index (CELL INDEX 48, 50).\n",
    "# We need playlist_genre for data_songs.index\n",
    "playlist_genres_for_W = data.loc[data_songs.index, 'playlist_genre'] # Usage of data.loc to ensure correct indexing\n",
    "clustered_data['playlist_genre'] = playlist_genres_for_W\n",
    "\n",
    "# # Create confusion matrix\n",
    "# # Get unique sorted genre names for consistent label ordering\n",
    "# genre_labels = sorted(clustered_data['playlist_genre'].unique())\n",
    "# cm_genre_df = pd.crosstab(clustered_data['playlist_genre'], clustered_data['kmeans_cluster'])\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(cm_genre_df, annot=True, fmt='d', cmap='Greens')\n",
    "# plt.title('Confusion Matrix: K-Means Clusters vs. Playlist Genres')\n",
    "# plt.ylabel('Actual Playlist Genre')\n",
    "# plt.xlabel('K-Means Cluster')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation :**\n",
    "\n",
    "L’analyse des clusters obtenus à partir de la **matrice W de la NMF** révèle des profils très contrastés :\n",
    "\n",
    "* Les **profils 1 à 4 sont clairement définis et homogènes** : ils se retrouvent **presque exclusivement dans un seul cluster (le cluster 3)**, ce qui indique une **cohérence interne forte** et une bonne séparation de ces profils par la NMF.\n",
    "\n",
    "* Le **profil 5** montre une **certaine diversité**, mais reste **partiellement concentré dans le cluster 1**, suggérant une structure intermédiaire : ni totalement homogène, ni complètement dispersé.\n",
    "\n",
    "* Le **profil 6** est **largement hétérogène**, avec des morceaux **répartis sur l’ensemble des 5 clusters**, ce qui confirme plusieurs points importants :\n",
    "\n",
    "  * Ce profil capture **un groupe de morceaux très variés**, sans dominante claire.\n",
    "  * Il reflète probablement **plusieurs sous-genres ou styles musicaux imbriqués**.\n",
    "  * Sa **taille très supérieure** à celle des autres profils suggère que la **NMF n’a pas réussi à découper efficacement cet ensemble**, ce qui peut être dû à une **grande diversité sonore** ou à une **structure non linéaire difficile à factoriser**.\n",
    "\n",
    "---\n",
    "\n",
    "**Bilan :**\n",
    "\n",
    "L’analyse met en évidence que **seul le profil 6 mérite une exploration plus approfondie**. On peut maintenant **zoomer sur ce profil**, et **interpréter les clusters qui le composent** en examinant :\n",
    "\n",
    "* La répartition des **genres** et **sous-genres musicaux**.\n",
    "* La structure des **caractéristiques audio** propres à chaque cluster.\n",
    "\n",
    "Cela permettra d’identifier si des **sous-groupes cohérents** émergent au sein de ce profil complexe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier le profil dominant pour chaque morceau\n",
    "dominant_profile = W.argmax(axis=1)  # Renvoie l'indice du profil dominant (0 à 5)\n",
    "\n",
    "# Filtrer uniquement ceux dont le profil dominant est le profil 6 (index 5)\n",
    "profile_6_indices = np.where(dominant_profile == 5)[0]\n",
    "W_profile_6 = W[profile_6_indices]\n",
    "\n",
    "kmeans = KMeans(init='k-means++', n_init='auto', random_state=42)\n",
    "visualizer = KElbowVisualizer(kmeans, k=(2, 12))\n",
    "visualizer.fit(W_profile_6)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer uniquement les données correspondant au profil 6\n",
    "profile_6_indices = np.where(dominant_profile == 5)[0]  # Indices des morceaux avec profil dominant 6\n",
    "W_profile_6 = W[profile_6_indices]  # Matrice W pour le profil 6\n",
    "data_songs_profile_6 = data_songs.iloc[profile_6_indices]  # Filtrer les morceaux correspondants\n",
    "\n",
    "# Effectuer le clustering K-Means sur W profil 6\n",
    "kmeans_profile_6 = KMeans(n_clusters=6, random_state=42, n_init='auto')\n",
    "cluster_labels_profile_6 = kmeans_profile_6.fit_predict(W_profile_6)\n",
    "\n",
    "# Ajouter les étiquettes de cluster au DataFrame filtré\n",
    "data_songs_profile_6['kmeans_cluster'] = cluster_labels_profile_6\n",
    "\n",
    "# Créer une matrice de contingence pour les genres de playlist\n",
    "playlist_genres_for_W_profile_6 = data.loc[data_songs_profile_6.index, 'playlist_genre']\n",
    "data_songs_profile_6['playlist_genre'] = playlist_genres_for_W_profile_6\n",
    "\n",
    "# Matrice de contingence : clusters K-Means vs genres de playlist\n",
    "cm_genre_df_profile_6 = pd.crosstab(data_songs_profile_6['playlist_genre'], data_songs_profile_6['kmeans_cluster'])\n",
    "\n",
    "# Visualisation de la matrice de contingence\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cm_genre_df_profile_6, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title('Confusion Matrix: K-Means Clusters vs. Playlist Genres (Profile 6)')\n",
    "plt.ylabel('Actual Playlist Genre')\n",
    "plt.xlabel('K-Means Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une matrice de contingence pour les sous-genres de playlist\n",
    "playlist_subgenres_for_W_profile_6 = data.loc[data_songs_profile_6.index, 'playlist_subgenre']\n",
    "data_songs_profile_6['playlist_subgenre'] = playlist_subgenres_for_W_profile_6\n",
    "\n",
    "# Matrice de contingence : clusters K-Means vs sous-genres de playlist\n",
    "cm_subgenre_df_profile_6 = pd.crosstab(data_songs_profile_6['playlist_subgenre'], data_songs_profile_6['kmeans_cluster'])\n",
    "\n",
    "# Visualisation de la matrice de contingence\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cm_subgenre_df_profile_6, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix: K-Means Clusters vs. Playlist Subgenres (Profile 6)')\n",
    "plt.ylabel('Actual Playlist Subgenre')\n",
    "plt.xlabel('K-Means Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Interprétation :**\n",
    "\n",
    "L’analyse des **clusters extraits du profil 6** (via NMF + k-means) révèle des résultats **globalement hétérogènes**, notamment lorsqu’on les compare aux **genres de playlists** : les morceaux sont **fortement dispersés entre les clusters**, ce qui rend l’interprétation difficile.\n",
    "\n",
    "Cependant, en se concentrant sur les **sous-genres musicaux**, **certains motifs apparaissent** et permettent de mieux comprendre la structure interne des clusters. Voici les principales observations :\n",
    "\n",
    "---\n",
    "\n",
    "##### 🎧 1. **Une dispersion marquée des morceaux Electro-House**\n",
    "\n",
    "Les morceaux d'**electro-house** (et dérivés) sont **présents dans presque tous les clusters**, **à l’exception du cluster 5**. Cela indique que :\n",
    "\n",
    "* Ces morceaux présentent **un large spectre de sonorités**, les rendant difficiles à regrouper de manière cohérente.\n",
    "* Cela suggère que la **NMF + k-means n’a pas réussi à isoler une structure claire** pour ce style, à la différence d’autres genres plus homogènes comme le hard-rock.\n",
    "\n",
    "---\n",
    "\n",
    "##### 🎤 2. **Clusters 0 et 3 : Des profils Rap différenciés**\n",
    "\n",
    "Ces deux clusters sont dominés par des **sous-genres du rap**, mais selon des nuances distinctes :\n",
    "\n",
    "* Le **cluster 0** contient principalement du **gangster rap** et du **southern hip-hop**, qui y sont **deux fois plus représentés** que les autres styles de rap.\n",
    "* Le **cluster 3**, quant à lui, est **plus équilibré** : on y trouve des parts similaires de **trap**, **hip-hop**, **southern hip-hop** et **gangster rap**, avec aussi une présence notable de **latin hip-hop**.\n",
    "\n",
    "Ces différences montrent que **le modèle distingue finement certaines nuances à l’intérieur même du spectre rap**.\n",
    "\n",
    "---\n",
    "\n",
    "##### 🎶 3. **Cluster 1 : Une dominante Pop sous un voile d’hétérogénéité**\n",
    "\n",
    "À première vue, le **cluster 1** semble très hétérogène. Toutefois, en examinant les sous-genres les plus fréquents, une tendance claire émerge :\n",
    "\n",
    "* On y retrouve majoritairement des styles tels que **pop**, **pop-rock**, **post-teen pop**, **dance pop**, **latin pop**, **pop-edm**, et **indie poptimism**.\n",
    "\n",
    "Cela s’explique par la **polyvalence intrinsèque des morceaux pop**, souvent présents dans des playlists de styles variés — une **caractéristique reflétée dans leur regroupement** ici.\n",
    "\n",
    "---\n",
    "\n",
    "##### 🎸 4. **Cluster 2 : Le noyau dur du Rock**\n",
    "\n",
    "Ce cluster est le **plus homogène** de tous :\n",
    "\n",
    "* Il regroupe en majorité des morceaux de **hard-rock**, ainsi que des variantes comme le **pop-rock**, **rock alternatif**, **indie poptimism**, ou encore **permanent wave**.\n",
    "\n",
    "Ce regroupement suggère que **les morceaux de rock (notamment hard) présentent des caractéristiques audio distinctes**, bien captées par la factorisation NMF et le groupement en clusters.\n",
    "\n",
    "---\n",
    "\n",
    "##### 🌙 5. **Cluster 5 : Des morceaux hybrides et alternatifs**\n",
    "\n",
    "Ce dernier cluster contient principalement des morceaux de type :\n",
    "\n",
    "* **indie poptimism**\n",
    "* **neo soul**\n",
    "* **urban contemporary**\n",
    "\n",
    "Il semble regrouper des morceaux **plus hybrides, subtils ou alternatifs**, peut-être à la **frontière entre plusieurs styles** — ce qui pourrait expliquer leur isolement dans ce cluster spécifique.\n",
    "\n",
    "---\n",
    "\n",
    "##### **Bilan** :\n",
    "\n",
    "Malgré une **forte hétérogénéité globale du profil 6 de la NMF**, l’analyse fine des **sous-genres** permet d’**extraire des structures locales** intéressantes. En particulier :\n",
    "\n",
    "* Les **morceaux de hard-rock** sont **clairement isolés**.\n",
    "* Les **raps** sont répartis mais **organisés selon des affinités fines**.\n",
    "* Les **morceaux pop**, bien que omniprésents, montrent une **cohérence interne dans leur dispersion**.\n",
    "* Enfin, la **présence massive de l’electro-house dans presque tous les clusters** met en lumière **les limites du k-means à capturer les spécificités de ce style à travers la projection de la NMF**, ou bien la **diversité sonore intrinsèque** de ce genre.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Algorithme de recommendation**\n",
    "Nous allons utiliser les resultats de la NMF pour créer un algorithme de recommandation basé sur le contenu. Cet algorithme va recommander des morceaux similaires à un morceau donné en se basant sur les profils musicaux identifiés par la NMF. Nous ferons de même avec une playlist donnée.\n",
    "\n",
    "#### Algorithme de recommandation de morceaux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "def recommend_similar_tracks(track_name, track_artist, num_recommendation=3):\n",
    "    # Trouver l'index de la chanson dans data_songs\n",
    "    song_idx = data_songs[\n",
    "        (data_songs['track_name'] == track_name) & \n",
    "        (data_songs['track_artist'] == track_artist)\n",
    "    ].index\n",
    "    \n",
    "    if len(song_idx) == 0:\n",
    "        return f\"Track '{track_name}' by '{track_artist}' not found in the dataset.\"\n",
    "    \n",
    "    song_idx = song_idx[0]\n",
    "    \n",
    "    # Récupérer le profil de la chanson dans profile_weights\n",
    "    song_profile = profile_weights.loc[song_idx].values.reshape(1, -1)\n",
    "    \n",
    "    # Calculer les similarités cosinus avec toutes les autres chansons\n",
    "    similarities = cosine_similarity(song_profile, profile_weights).flatten()\n",
    "    \n",
    "    # Trier les indices des chansons par similarité décroissante\n",
    "    similar_indices = similarities.argsort()[::-1]\n",
    "    \n",
    "    # Exclure la chanson elle-même et prendre les num_recommendation plus proches\n",
    "    similar_indices = [idx for idx in similar_indices if idx != song_idx][1:num_recommendation+1]\n",
    "    \n",
    "    # Récupérer les informations des chansons similaires\n",
    "    similar_tracks = data_songs.iloc[similar_indices][['track_name', 'track_artist']]\n",
    "    similar_tracks['similarity'] = similarities[similar_indices]\n",
    "    \n",
    "    return similar_tracks.reset_index(drop=True)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "recommend_similar_tracks(\"Thunderstruck\", \"AC/DC\", num_recommendation=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "def recommend_similar_tracks_playlist(playlist_name, num_recommendation=3):\n",
    "    # Trouver l'index de la chanson dans data_songs\n",
    "    song_idx = data_songs[\n",
    "        (data_songs['track_name'] == track_name) & \n",
    "        (data_songs['track_artist'] == track_artist)\n",
    "    ].index\n",
    "    \n",
    "    if len(song_idx) == 0:\n",
    "        return f\"Track '{track_name}' by '{track_artist}' not found in the dataset.\"\n",
    "    \n",
    "    song_idx = song_idx[0]\n",
    "    \n",
    "    # Récupérer le profil de la chanson dans profile_weights\n",
    "    song_profile = profile_weights.loc[song_idx].values.reshape(1, -1)\n",
    "    \n",
    "    # Calculer les similarités cosinus avec toutes les autres chansons\n",
    "    similarities = cosine_similarity(song_profile, profile_weights).flatten()\n",
    "    \n",
    "    # Trier les indices des chansons par similarité décroissante\n",
    "    similar_indices = similarities.argsort()[::-1]\n",
    "    \n",
    "    # Exclure la chanson elle-même et prendre les num_recommendation plus proches\n",
    "    similar_indices = [idx for idx in similar_indices if idx != song_idx][:num_recommendation]\n",
    "    \n",
    "    # Récupérer les informations des chansons similaires\n",
    "    similar_tracks = data_songs.iloc[similar_indices][['track_name', 'track_artist']]\n",
    "    similar_tracks['similarity'] = similarities[similar_indices]\n",
    "    \n",
    "    return similar_tracks.reset_index(drop=True)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "recommend_similar_tracks(\"Thunderstruck\", \"AC/DC\", num_recommendation=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
